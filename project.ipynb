{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "The dataset we are using is comprised of 2.5 years of insulin dosage, blood glucose (bg), and Estimated Variability of Glucose (EVG). This data was collected from a type 1 diabetic's insulin pump and continuous glucose monitor (CGM). Unfortunately this data isnt entirely continous over the 2.5 year span and has gaps in some instances. This data has been exported directly from 30 day span .csv files containing 3 tables one for insulin dosage (bolus), bg, and EVG data. We split these files into the 3 respective tables and saved them to new tables containing the full time-span of data, these files are titles Bolus.csv, BG.csv, and EVG.csv. Here I will detail the contents of these files.\n",
    "\n",
    "1. Bolus\n",
    "    * Features:\n",
    "        * Type: Type of bolus event (Always 'Bolus').\n",
    "        * BolusType: Describes the wayin which the Bolus was used (categorical).\n",
    "        * BolusDeliveryMethod: Method used to deliver the bolus (Auto or Standard)(categorical).\n",
    "        * BG (mg/dL): Blood glucose levels at the time of bolus administration (continuous data).\n",
    "        * SerialNumber: Device identifier.\n",
    "        * CompletionDateTime: Timestamp when the dosing was completed.\n",
    "        * InsulinDelivered: The standard unit measure for the amount of insulin delivered (continuous data).\n",
    "        * FoodDelivered: Insulin delivered for food consumption (continuous data).\n",
    "        * CorrectionDelivered: Insulin delivered for BG correction (continuous data).\n",
    "        * CompletionStatusDesc: Status description of the dosage (categorical).\n",
    "        * BolexStartDateTime: Not Used In Export.\n",
    "        * BolexCompletionDateTime: Not Used In Export.\n",
    "        * BolexInsulinDelivered: Not Used In Export.\n",
    "        * BolexCompletionStatusDesc: Not Used In Export.\n",
    "        * StandardPercent: (Always 100).\n",
    "        * Duration (mins):(Always 0).\n",
    "        * CarbSize: The amount of carbohydrates consumed in grams (continuous data).\n",
    "        * TargetBG (mg/dL): Target blood glucose level for the subject in milligrams per deciliter (continuous data).\n",
    "        * CorrectionFactor: Insulin sensitivity factor (continuous data).\n",
    "        * CarbRatio: Insulin-to-carbohydrate ratio (continuous data).\n",
    "    * Notes:\n",
    "        * This table holds the most detailed records for insulin dosing decisions, food intake, and blood glucose corrections.\n",
    "        * This table will be very useful in modeling relationships between carbohydrate intake, insulin dosage, and BG levels.\n",
    "        * Bolus (Def: A large single doseage of insuline to lower a bloodsugar rise) refers to the device/method of insuline delivery. Which is automated through a pocket size device with a refillable tank of insulin.\n",
    "2. EVG Table\n",
    "    * Features:\n",
    "        * DeviceType: Type/Name of the device used to record the event.\n",
    "        * SerialNumber: Device identifier.\n",
    "        * Description: A text description of the type of data recorded (Always EVG).\n",
    "        * EventDateTime: Timestamp of when the measurement was recorded.\n",
    "        * Readings (mg/dL): The estimated glucose level at the recorded time, in milligrams per deciliter (mg/dL).\n",
    "    * Notes: \n",
    "        * The EVG data is collected directly from a CGM.\n",
    "        * EVG is designed for tracking overall trends and patterns in glucose levels rather than moment-to-moment decisions.\n",
    "        * Useful for identifying time-in-range, glucose variability, and predicting hypo/hyperglycemia over time.\n",
    "        \n",
    "3. BG Table\n",
    "    * Features:\n",
    "        * DeviceType: Type/Name of device used to measure blood glucose.\n",
    "        * SerialNumber: Device identifier.\n",
    "        * Description: A text description of the type of data recorded (Always BG)\n",
    "        * EventDateTime: Timestamp of the blood glucose measurement.\n",
    "        * BG (mg/dL): Blood glucose levels in milligrams per deciliter (continuous data).\n",
    "        * Note: Additional notes field (Always Blank).\n",
    "    * Notes:\n",
    "        * This data is manually entered and is recorded through a glucometer (Finger Prick).\n",
    "        * The BG measurement reflects the actual glucose levels at the time of measurement.\n",
    "\n",
    "\n",
    "### General Dataset Properties\n",
    "\n",
    "* Total \\# of Features:\n",
    "    * Bolus: 18 features\n",
    "    * EVG: 5 features\n",
    "    * BG: 6 features\n",
    "* \\# of Usable/Useful Features: (!!!!! Update possibly)\n",
    "    * Bolus: 12 features\n",
    "    * EVG: 2 features\n",
    "    * BG: 2 features\n",
    "* Table Sizes:\n",
    "    * Bolus: 11,348 Records\n",
    "    * EVG: 191,781 Records\n",
    "    * BG: 2,832 Records\n",
    "* Unique Dates: \n",
    "    * Bolus: 599 Days\n",
    "    * EVG: 666 Days\n",
    "    * BG: 663 Days\n",
    "* Intersecting Dates: 599\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "Our goal for this porject is to create a machine learning model that is capable of predicting blood glucose levels based on past data. Given that this data is mostly continous and can be used in such manner, we have decided to employ the use of a Recurrent Neural Network (RNN). Seeing how we want to use a lot of data from a varying timespans, the drawbacks of Short-Term memory may produce less accurate predictions. To remedy this we decided to focus on the use of a RNN variant called Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "1. Research and Model Selection\n",
    "    (***ADD Writing***)\n",
    "\n",
    "1. Problem Framing and Input Variables\n",
    "(***Update After Finishing***)\n",
    "The predictive task involves forecasting BG levels using historical data from the Bolus dataset. We selected the following features as inputs to the model based on their availability, continuity, and relevance to BG level prediction:\n",
    "    * BG (mg/dL): Blood glucose levels at specific times.\n",
    "    * InsulinDelivered: The amount of insulin delivered.\n",
    "    * FoodDelivered: Insulin dose related to food intake.\n",
    "    * CarbSize: Amount of carbohydrates consumed.\n",
    "Reasoning:\n",
    "    * These features are continuous (non-categorical) and align with domain knowledge of factors influencing BG levels.\n",
    "    * The Bolus dataset provides a large number of entries, allowing for robust training of the model.\n",
    "To frame the input-output relationship, we structured the input data as a time window of the past 24 hours, using these features to predict BG levels at a future timestamp. We also plan to experiment with varying the length of the input window to optimize model accuracy.\n",
    "\n",
    "\n",
    "1. Data Preprocessing\n",
    "Steps for preparing the data for the LSTM model:\n",
    "    (***ADD Writing***)\n",
    "    * Data Cleaning:\n",
    "        * Handle missing values and fill gaps in time-series data, ensuring continuity.\n",
    "        * Filter out irrelevant or unused features to streamline inputs.\n",
    "    * Feature Scaling:\n",
    "        * Normalize all input features to ensure consistent scaling, which is critical for LSTM performance.\n",
    "    * Windowing:\n",
    "        * Create overlapping sequences of the past 24 hours' data to use as input features, with the BG level at the next timestamp as the target variable.\n",
    "    * Train-Test Split:\n",
    "        * Split the data into training and testing sets while preserving time order to prevent data leakage.\n",
    "        \n",
    "1. Model Implementation\n",
    "We used the Keras library to implement the LSTM model, referencing a multivariate time-series forecasting tutorial as a guide. The key steps involved:\n",
    "    * Model Architecture:\n",
    "        * A sequential LSTM-based model was constructed with:\n",
    "        * Input layers to process multivariate time-series data.\n",
    "        * LSTM layers for feature extraction and sequence learning.\n",
    "        * Dense layers for final predictions of BG levels.\n",
    "    * Hyperparameter Selection:\n",
    "        * We plan to fine-tune key hyperparameters, including:\n",
    "        * Number of LSTM units.\n",
    "        * Batch size.\n",
    "        * Learning rate.\n",
    "        * Number of time steps (length of the input sequence).\n",
    "\n",
    "1. Experimental Design\n",
    "To ensure the model performs well, we will:\n",
    "    * Hyperparameter Optimization:\n",
    "        * Use techniques like grid search or randomized search to identify optimal model parameters.\n",
    "    * Validation:\n",
    "        * Implement cross-validation to evaluate model performance.\n",
    "    * Error Analysis:\n",
    "        * Examine predictions and residuals to understand model strengths and weaknesses.\n",
    "        * Feature Experimentation:\n",
    "        * Test the inclusion or exclusion of additional features (e.g., CorrectionDelivered, TargetBG) to assess their impact on performance.\n",
    "\n",
    "1. Planned Outputs\n",
    "The final outputs will include:\n",
    "    * Predicted BG levels over time.\n",
    "    * Performance metrics such as mean squared error (MSE) and mean absolute error (MAE).\n",
    "    * Visualizations comparing predicted BG levels to actual values for test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Notes\n",
    "* Not all Days are covered, at points there are significant (5-8 week) gaps in data.\n",
    "* Different Devices used to collect data may overlap times (device marked as 'UNKNOWN' in data)\n",
    "* 3 different tables (I think our main focus should be on the third table that has the pump data coupled with the best of the BG tables (1 or 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Table 1 DataFrame:\n",
      "  DeviceType SerialNumber Description        EventDateTime Readings (mg/dL)\n",
      "0    Unknown       870772         EGV  2023-03-01T00:00:55              174\n",
      "1    Unknown       870772         EGV  2023-03-01T00:05:55              172\n",
      "2    Unknown       870772         EGV  2023-03-01T00:10:55              171\n",
      "3    Unknown       870772         EGV  2023-03-01T00:15:55              168\n",
      "4    Unknown       870772         EGV  2023-03-01T00:20:55              165\n",
      "\n",
      "Table 2 DataFrame:\n",
      "  DeviceType SerialNumber Description        EventDateTime BG (mg/dL) Note\n",
      "0    Unknown       870772          BG  2023-03-01T16:07:59        238     \n",
      "1    Unknown       870772          BG  2023-03-01T18:04:52        382     \n",
      "2    Unknown       870772          BG  2023-03-01T20:09:14        156     \n",
      "3    Unknown       870772          BG  2023-03-01T20:31:14        248     \n",
      "4    Unknown       870772          BG  2023-03-02T12:46:15        157     \n",
      "\n",
      "Table 3 DataFrame:\n",
      "    Type BolusType BolusDeliveryMethod BG (mg/dL) SerialNumber  \\\n",
      "0  Bolus      Auto                Auto          0       870772   \n",
      "1  Bolus      Auto                Auto          0       870772   \n",
      "2  Bolus      Auto                Auto          0       870772   \n",
      "3  Bolus      Auto                Auto          0       870772   \n",
      "4  Bolus      Auto                Auto          0       870772   \n",
      "\n",
      "    CompletionDateTime InsulinDelivered FoodDelivered CorrectionDelivered  \\\n",
      "0  2023-03-01T01:02:24             0.65             0                   0   \n",
      "1  2023-03-01T02:12:13             1.24             0                   0   \n",
      "2  2023-03-01T03:22:44             0.64             0                   0   \n",
      "3  2023-03-01T07:48:06             0.62             0                   0   \n",
      "4  2023-03-01T08:53:18             1.01             0                   0   \n",
      "\n",
      "  CompletionStatusDesc BolexStartDateTime BolexCompletionDateTime  \\\n",
      "0            Completed                                              \n",
      "1            Completed                                              \n",
      "2            Completed                                              \n",
      "3            Completed                                              \n",
      "4            Completed                                              \n",
      "\n",
      "  BolexInsulinDelivered BolexCompletionStatusDesc StandardPercent  \\\n",
      "0                                                             100   \n",
      "1                                                             100   \n",
      "2                                                             100   \n",
      "3                                                             100   \n",
      "4                                                             100   \n",
      "\n",
      "  Duration (mins) CarbSize TargetBG (mg/dL) CorrectionFactor CarbRatio  \n",
      "0               0        0              110               65       0.0  \n",
      "1               0        0              110               65       0.0  \n",
      "2               0        0              110               65       0.0  \n",
      "3               0        0              110               65       0.0  \n",
      "4               0        0              110               65       0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def load_export_csv(file_path):\n",
    "    # Define the headers for each table to identify them\n",
    "    table_headers = {\n",
    "        'table1': [\"DeviceType\", \"SerialNumber\", \"Description\", \"EventDateTime\", \"Readings (mg/dL)\"],\n",
    "        'table2': [\"DeviceType\", \"SerialNumber\", \"Description\", \"EventDateTime\", \"BG (mg/dL)\", \"Note\"],\n",
    "        'table3': [\"Type\", \"BolusType\", \"BolusDeliveryMethod\", \"BG (mg/dL)\", \"SerialNumber\",\n",
    "                   \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CorrectionDelivered\",\n",
    "                   \"CompletionStatusDesc\", \"BolexStartDateTime\", \"BolexCompletionDateTime\",\n",
    "                   \"BolexInsulinDelivered\", \"BolexCompletionStatusDesc\", \"StandardPercent\",\n",
    "                   \"Duration (mins)\", \"CarbSize\", \"TargetBG (mg/dL)\", \"CorrectionFactor\",\n",
    "                   \"CarbRatio\"]\n",
    "    }\n",
    "    \n",
    "    # Initialize data storage for each table\n",
    "    data_tables = {\n",
    "        'table1': [],\n",
    "        'table2': [],\n",
    "        'table3': []\n",
    "    }\n",
    "    \n",
    "    current_table = None  # To keep track of which table we're currently reading\n",
    "    line_number = 0  # To track line numbers for debugging\n",
    "    \n",
    "    with open(file_path, 'r', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            # Strip whitespace from each cell\n",
    "            row = [cell.strip() for cell in row]\n",
    "            \n",
    "            # Debugging: Print current row and line number\n",
    "            # print(f\"Line {line_number}: {row}\")\n",
    "            \n",
    "            # Check if the row matches any table header\n",
    "            if row[:len(table_headers['table1'])] == table_headers['table1']:\n",
    "                current_table = 'table1'\n",
    "                # print(f\"Detected header for table1 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif row[:len(table_headers['table2'])] == table_headers['table2']:\n",
    "                current_table = 'table2'\n",
    "                # print(f\"Detected header for table2 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif row[:len(table_headers['table3'])] == table_headers['table3']:\n",
    "                current_table = 'table3'\n",
    "                # print(f\"Detected header for table3 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif not any(cell for cell in row):\n",
    "                # Empty row signifies possible separation; skip\n",
    "                current_table = None\n",
    "                # print(f\"Detected empty row at line {line_number}; resetting current_table\")\n",
    "                continue\n",
    "            \n",
    "            # If current_table is set, append the row to the corresponding data list\n",
    "            if current_table:\n",
    "                expected_columns = len(table_headers[current_table])\n",
    "                actual_columns = len(row)\n",
    "                \n",
    "                if actual_columns != expected_columns:\n",
    "                    print(f\"Warning: Line {line_number} has {actual_columns} columns, expected {expected_columns}. Skipping row.\")\n",
    "                    continue  # Skip rows that don't match the expected column count\n",
    "                \n",
    "                # Replace '(Data)' placeholders with actual data if necessary\n",
    "                cleaned_row = [cell if cell != '(Data)' else None for cell in row]\n",
    "                data_tables[current_table].append(cleaned_row)\n",
    "            else:\n",
    "                # Rows outside of any table headers are ignored\n",
    "                print(f\"Warning: Line {line_number} is outside of any table. Skipping row.\")\n",
    "                continue\n",
    "    \n",
    "    # Convert lists to DataFrames with appropriate columns\n",
    "    df_tables = {}\n",
    "    for table_key, data in data_tables.items():\n",
    "        if data:  # Only create DataFrame if there's data\n",
    "            df = pd.DataFrame(data, columns=table_headers[table_key])\n",
    "            df_tables[table_key] = df\n",
    "        else:\n",
    "            df_tables[table_key] = pd.DataFrame(columns=table_headers[table_key])\n",
    "    \n",
    "    return df_tables['table1'], df_tables['table2'], df_tables['table3']\n",
    "\n",
    "\n",
    "\n",
    "file_path = './Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920-2.csv'  \n",
    "df_table1, df_table2, df_table3 = load_export_csv(file_path)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Table 1 DataFrame:\")\n",
    "print(df_table1.head())\n",
    "\n",
    "print(\"\\nTable 2 DataFrame:\")\n",
    "print(df_table2.head())\n",
    "\n",
    "print(\"\\nTable 3 DataFrame:\")\n",
    "print(df_table3.head())\n",
    "\n",
    "# Optionally, save the DataFrames to separate CSV files\n",
    "df_table1.to_csv(os.path.join('./DataTables', 'table_1.csv'), index=False)\n",
    "df_table2.to_csv(os.path.join('./DataTables', 'table_2.csv'), index=False)\n",
    "df_table3.to_csv(os.path.join('./DataTables', 'table_3.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def gather_csv_files(main_directory):\n",
    "    \"\"\"\n",
    "    Gathers all CSV files from subdirectories (2022PumpData, 2023PumpData, 2024PumpData)\n",
    "    in the 'data' directory located in the main directory.\n",
    "\n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of paths to all CSV files found in the specified subdirectories.\n",
    "    \"\"\"\n",
    "    # Define the parent directory\n",
    "    parent_directory = os.path.join(main_directory, 'Data')\n",
    "    \n",
    "    # List of subdirectories to search\n",
    "    subdirectories = ['2022PumpData', '2023PumpData', '2024PumpData']\n",
    "    \n",
    "    # Collect CSV files from all subdirectories\n",
    "    csv_files = []\n",
    "    for subdir in subdirectories:\n",
    "        subdir_path = os.path.join(parent_directory, subdir)\n",
    "        print(subdir_path)\n",
    "        csv_files.extend(glob.glob(os.path.join(subdir_path, '*.csv')))\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# Usage example\n",
    "main_directory = './'  # Replace with the path to your main directory\n",
    "csv_files = gather_csv_files(main_directory)\n",
    "\n",
    "# Print the gathered CSV file paths\n",
    "for file in csv_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_all_csv_files_combined(main_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the 'Data/2022PumpData', 'Data/2023PumpData', and 'Data/2024PumpData'\n",
    "    subdirectories, extracting tables and appending them into combined CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory containing the 'Data' folder.\n",
    "        output_directory (str): The path to the directory where combined tables will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Gather all CSV files\n",
    "    csv_files = gather_csv_files(main_directory)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Initialize empty DataFrames for combined output\n",
    "    combined_table1 = pd.DataFrame()\n",
    "    combined_table2 = pd.DataFrame()\n",
    "    combined_table3 = pd.DataFrame()\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file and extract tables\n",
    "            df_table1, df_table2, df_table3 = load_export_csv(file_path)\n",
    "            \n",
    "            # Append each table to its respective combined DataFrame\n",
    "            combined_table1 = pd.concat([combined_table1, df_table1], ignore_index=True)\n",
    "            combined_table2 = pd.concat([combined_table2, df_table2], ignore_index=True)\n",
    "            combined_table3 = pd.concat([combined_table3, df_table3], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Define output file paths\n",
    "    table1_file = os.path.join(output_directory, 'EVG.csv')\n",
    "    table2_file = os.path.join(output_directory, 'BG.csv')\n",
    "    table3_file = os.path.join(output_directory, 'Bolus.csv')\n",
    "\n",
    "    # Save the combined tables to CSV\n",
    "    combined_table1.to_csv(table1_file, index=False)\n",
    "    combined_table2.to_csv(table2_file, index=False)\n",
    "    combined_table3.to_csv(table3_file, index=False)\n",
    "\n",
    "    print(f\"Combined tables saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "main_directory = './'  # Replace with the path to your main directory\n",
    "output_directory = './DataTables'  # Replace with your desired output directory\n",
    "\n",
    "process_all_csv_files_combined(main_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "Number of unique dates that intersect across all three tables: 599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "file_path1 = './DataTables/BG.csv'  # Replace with the actual path for Table 1\n",
    "file_path2 = './DataTables/EVG.csv'  # Replace with the actual path for Table 2\n",
    "file_path3 = './DataTables/Bolus.csv'  # Replace with the actual path for Table 3\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df3 = pd.read_csv(file_path3)\n",
    "\n",
    "# Convert the EventDateTime column to datetime and extract dates\n",
    "df1['EventDate'] = pd.to_datetime(df1['EventDateTime']).dt.date\n",
    "df2['EventDate'] = pd.to_datetime(df2['EventDateTime']).dt.date\n",
    "df3['EventDate'] = pd.to_datetime(df3['CompletionDateTime']).dt.date\n",
    "\n",
    "# Find the unique dates for each table\n",
    "unique_dates1 = set(df1['EventDate'].unique())\n",
    "unique_dates2 = set(df2['EventDate'].unique())\n",
    "unique_dates3 = set(df3['EventDate'].unique())\n",
    "\n",
    "print(len(unique_dates3))\n",
    "\n",
    "# Find the intersection of unique dates across all three tables\n",
    "common_dates = unique_dates1.intersection(unique_dates2).intersection(unique_dates3)\n",
    "\n",
    "# Print the number of unique intersecting dates\n",
    "print(f\"Number of unique dates that intersect across all three tables: {len(common_dates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1583064809.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def date_handling()\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Time Based Features\n",
    "Modify usage as needed.\n",
    "\"\"\"\n",
    "\n",
    "def date_handling(df, dfName):\n",
    "    field = ''\n",
    "    if dfName == 'Bolus':\n",
    "        field = 'CompletionDateTime'\n",
    "    if (dfName == 'BG') or (dfName == 'EVG'):\n",
    "        field = 'EventDateTime'    \n",
    "    df = seconds_since_epoch(df, field)\n",
    "    df = time_of_day(df, field)\n",
    "    df = day_of_week(df, field)\n",
    "    df = week(df, field)\n",
    "    df = month(df, field)\n",
    "    return df\n",
    "\n",
    "def seconds_since_epoch(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "    df['CompletionTimeSec'] = df[field].astype('int64') // 10**9\n",
    "    return df\n",
    "\n",
    "\"\"\" time_of_day():\n",
    "    We may only want to show morning, afternoon, night?\n",
    "\"\"\"\n",
    "def time_of_day(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "\n",
    "    df['TimeOfDaySec'] = (\n",
    "        df[field].dt.hour * 3600 + \n",
    "        df[field].dt.minute * 60 + \n",
    "        df[field].dt.second\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def day_of_week(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "    df['DayOfWeek'] = df[field].dt.weekday\n",
    "    return df\n",
    "\n",
    "def week(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "    df['Week'] = df[field].dt.isocalendar().week\n",
    "    return df\n",
    "\n",
    "def month(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "    df['Month'] = df[field].dt.month\n",
    "    return df\n",
    "\n",
    "def day_of_year(df, field):\n",
    "    df[field] = pd.to_datetime(df[field])\n",
    "    df['DayOfYear'] = df[field].dt.dayofyear\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\"\"\"\n",
    "def onehot_encoding(df):\n",
    "    df = pd.get_dummies(df, columns=['BolusType', 'BolusDeliveryMethod', 'CompletionStatusDesc'])\n",
    "    return df\n",
    "\n",
    "def label_encoding(df):\n",
    "    bolus_type_encoder = LabelEncoder()\n",
    "    delivery_method_encoder = LabelEncoder()\n",
    "    status_desc_encoder = LabelEncoder()\n",
    "\n",
    "    # Apply label encoding\n",
    "    df['BolusType_Encoded'] = bolus_type_encoder.fit_transform(df['BolusType'])\n",
    "    df['BolusDeliveryMethod_Encoded'] = delivery_method_encoder.fit_transform(df['BolusDeliveryMethod'])\n",
    "    df['CompletionStatusDesc_Encoded'] = status_desc_encoder.fit_transform(df['CompletionStatusDesc'])\n",
    "    return df\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    ETHAN: I am seeing some things on embedding encodings in an embedding layer for the LTSM model, I believe it will do it automatically if you are using Embeddings() after label encoding the data\n",
    "    \n",
    "    from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "def min_max_scaling(df, field):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    fieldNew = \"Scaled\" + field\n",
    "    df[fieldNew] = scaler.fit_transform(df[[field]])\n",
    "    return df\n",
    "\n",
    "def standard_scaling(df, field):\n",
    "    scaler = StandardScaler()\n",
    "    fieldNew = \"Scaled\" + field\n",
    "    df[fieldNew] = scaler.fit_transform(df[[field]])\n",
    "    return df\n",
    "\n",
    "def robust_scaling(df, field):\n",
    "    scaler = RobustScaler()\n",
    "    fieldNew = \"Scaled\" + field\n",
    "    df[fieldNew] = scaler.fit_transform(df[[field]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: False\n",
      "[[0.         0.26737968 0.26737968 ... 1.         0.         0.        ]\n",
      " [0.22       0.10481283 0.08930481 ... 1.         0.         0.        ]\n",
      " [0.         0.05721925 0.05721925 ... 1.         0.         0.        ]\n",
      " ...\n",
      " [0.475      0.         0.         ... 0.         0.         0.        ]\n",
      " [0.49666667 0.0197861  0.         ... 1.         0.         0.        ]\n",
      " [0.43166667 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "def round_to_nearest_5_minutes(df):\n",
    "    # Ensure CompletionDateTime is in datetime format\n",
    "    df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'], errors='coerce')\n",
    "    # Drop rows with invalid datetime entries\n",
    "    df = df.dropna(subset=['CompletionDateTime'])\n",
    "    # Round CompletionDateTime to the nearest 5 minutes\n",
    "    df['CompletionDateTime'] = df['CompletionDateTime'].dt.round('5min')\n",
    "    return df\n",
    "\n",
    "def remove_cgm_on_time_overlap(df):\n",
    "    # Identify duplicates based on CompletionDateTime\n",
    "    duplicate_times = df[df.duplicated('CompletionDateTime', keep=False)]\n",
    "    # Filter out rows where Type is EVG and CompletionDateTime is duplicated\n",
    "    df = df[~((df['CompletionDateTime'].isin(duplicate_times['CompletionDateTime'])) & (df['Type'] == 'CGM'))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_data(df):\n",
    "    # Example Bolus table columns\n",
    "    bolus_columns = [\n",
    "        'Type', 'BolusType', 'BolusDeliveryMethod', 'BG (mg/dL)', 'SerialNumber',\n",
    "        'CompletionDateTime', 'InsulinDelivered', 'FoodDelivered', 'CorrectionDelivered',\n",
    "        'CompletionStatusDesc', 'BolexStartDateTime', 'BolexCompletionDateTime', \n",
    "        'BolexInsulinDelivered', 'BolexCompletionStatusDesc', 'StandardPercent', \n",
    "        'Duration (mins)', 'CarbSize', 'TargetBG (mg/dL)', 'CorrectionFactor', 'CarbRatio'\n",
    "    ]\n",
    "\n",
    "    # 1. Create a template DataFrame for Bolus table with default values\n",
    "    template_bolus = pd.DataFrame(columns=bolus_columns)\n",
    "\n",
    "    # Default values for missing fields\n",
    "    default_numeric = 0\n",
    "    default_categorical = ''\n",
    "\n",
    "    # 2. Map EVG fields to Bolus fields\n",
    "    mapped_bolus = evg_df.rename(columns={\n",
    "        'Readings (mg/dL)': 'BG (mg/dL)',\n",
    "        'SerialNumber': 'SerialNumber',\n",
    "        'EventDateTime': 'CompletionDateTime'\n",
    "    })\n",
    "\n",
    "    # 3. Add missing columns with default values\n",
    "    for col in bolus_columns:\n",
    "        if col not in mapped_bolus.columns:\n",
    "            if col in ['BG (mg/dL)', 'InsulinDelivered', 'FoodDelivered', 'CorrectionDelivered', \n",
    "                    'StandardPercent', 'Duration (mins)', 'CarbSize', 'TargetBG (mg/dL)', \n",
    "                    'CorrectionFactor', 'CarbRatio']:  # Numeric fields\n",
    "                mapped_bolus[col] = default_numeric\n",
    "            elif col == 'Type':\n",
    "                mapped_bolus[col] = 'CGM'\n",
    "            else:  # Categorical fields\n",
    "                mapped_bolus[col] = default_categorical\n",
    "\n",
    "    # 4. Reorder columns to match the Bolus table structure\n",
    "    mapped_bolus = mapped_bolus[bolus_columns]\n",
    "\n",
    "    # Resulting Bolus DataFrame\n",
    "    #print(mapped_bolus)\n",
    "    return mapped_bolus\n",
    "\n",
    "\n",
    "def min_max_scaling(df, field):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    fieldNew = \"Scaled\" + field\n",
    "    df[fieldNew] = scaler.fit_transform(df[[field]])\n",
    "    return df\n",
    "\n",
    "def map_categories_to_indices(df, columns):\n",
    "    \"\"\"\n",
    "    Map categorical values to numeric indices.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[f\"{col}_idx\"] = df[col].cat.codes  # Create numeric indices\n",
    "        mappings[col] = dict(enumerate(df[col].cat.categories))  # Save mappings\n",
    "    return df, mappings\n",
    "\n",
    "def onehot_encoding(df, columns):\n",
    "    df = pd.get_dummies(df, columns=columns)\n",
    "    return df\n",
    "\n",
    "def onehot_encoding_with_all_categories(df, categorical_columns, all_possible_categories):\n",
    "    \"\"\"\n",
    "    One-hot encode specified categorical columns, ensuring all possible categories are included.\n",
    "    \"\"\"\n",
    "    for col in categorical_columns:\n",
    "        df[col] = pd.Categorical(df[col], categories=all_possible_categories[col])\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, prefix=categorical_columns, dummy_na=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "#file_path1 = './DataTables/BG.csv'  # Replace with the actual path for Table 1\n",
    "file_path2 = './DataTables/EVG.csv'  # Replace with the actual path for Table 2\n",
    "file_path3 = './DataTables/Bolus.csv'  # Replace with the actual path for Table 3\n",
    "\n",
    "# Load the data\n",
    "#df1 = pd.read_csv(file_path1)\n",
    "evg_df = pd.read_csv(file_path2)\n",
    "bolus_df = pd.read_csv(file_path3)\n",
    "\n",
    "evg_mapped = convert_data(evg_df)\n",
    "\n",
    "df = pd.concat([bolus_df, evg_mapped], ignore_index=True)\n",
    "#print(evg_mapped)\n",
    "\n",
    "# Round CompletionDateTime to the nearest 5 minutes\n",
    "df = round_to_nearest_5_minutes(df)\n",
    "\n",
    "# Sort the DataFrame by CompletionDateTime\n",
    "df = df.sort_values(by='CompletionDateTime').reset_index(drop=True)\n",
    "\n",
    "df = remove_cgm_on_time_overlap(df)\n",
    "columns=['Type', 'BolusType', 'BolusDeliveryMethod', 'CompletionStatusDesc']\n",
    "# Apply the function\n",
    "df = onehot_encoding(df, columns)\n",
    "columns=['SerialNumber', 'BolexStartDateTime', 'BolexCompletionDateTime', 'BolexInsulinDelivered', 'BolexCompletionStatusDesc']\n",
    "df = df.drop(columns=columns)\n",
    "bool_columns = df.select_dtypes(include='bool').columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "df.rename(columns={'CompletionDateTime': 'Date'}, inplace=True)\n",
    "\n",
    "# Move the 'Date' column to the first position\n",
    "first_col = df.pop('Date')  # Remove 'Date' column\n",
    "df.insert(0, 'Date', first_col)  # Reinsert it at position 0\n",
    "\n",
    "# Display the DataFrame\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "df.to_csv('test.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check for NaN values; there are none\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "# Organize data, make date into an index\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "# Note: we are not using any categorical data\n",
    "values = df.values\n",
    "# specify columns to plot\n",
    "#groups = [0, 1, 2, 3]\n",
    "#i = 1\n",
    "# plot each column\n",
    "#pyplot.figure()\n",
    "#for group in groups:\n",
    "#\tpyplot.subplot(len(groups), 1, i)\n",
    "#\tpyplot.plot(values[:, group])\n",
    "#\tpyplot.title(df.columns[group], y=0.5, loc='right')\n",
    "#\ti += 1\n",
    "#pyplot.show()\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Type  CompletionDateTime  BG (mg/dL)  InsulinDelivered\n",
      "0    EVG 2022-05-26 06:25:00         137               0.0\n",
      "1  Bolus 2022-05-26 06:25:00         140               4.5\n",
      "2    EVG 2022-05-26 10:10:00          99               0.0\n",
      "3  Bolus 2022-05-26 10:10:00         100               3.8\n",
      "4    EVG 2022-05-26 11:15:00         138               0.0\n",
      "    Type  CompletionDateTime  BG (mg/dL)  InsulinDelivered\n",
      "1  Bolus 2022-05-26 06:25:00         140               4.5\n",
      "3  Bolus 2022-05-26 10:10:00         100               3.8\n",
      "4    EVG 2022-05-26 11:15:00         138               0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample test data\n",
    "test_data = {\n",
    "    'Type': ['EVG', 'Bolus', 'EVG', 'Bolus', 'EVG'],\n",
    "    'CompletionDateTime': [\n",
    "        '2022-05-26 06:23:59',\n",
    "        '2022-05-26 06:25:00',\n",
    "        '2022-05-26 10:08:17',\n",
    "        '2022-05-26 10:10:00',\n",
    "        '2022-05-26 11:16:39'\n",
    "    ],\n",
    "    'BG (mg/dL)': [137, 140, 99, 100, 138],\n",
    "    'InsulinDelivered': [0, 4.5, 0, 3.8, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Ensure CompletionDateTime is datetime\n",
    "test_df['CompletionDateTime'] = pd.to_datetime(test_df['CompletionDateTime'])\n",
    "\n",
    "# Apply rounding to the nearest 5 minutes\n",
    "test_df['CompletionDateTime'] = test_df['CompletionDateTime'].dt.round('5min')\n",
    "\n",
    "# Sort by CompletionDateTime\n",
    "test_df = test_df.sort_values(by='CompletionDateTime').reset_index(drop=True)\n",
    "print(test_df)\n",
    "# Remove EVG records with overlapping times\n",
    "test_df = remove_cgm_on_time_overlap(test_df)\n",
    "\n",
    "# Print the result\n",
    "print(test_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
