{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dataset\n",
    "## Data Description\n",
    "The dataset we are using is comprised of 2.5 years of insulin dosage, blood glucose (bg), and Estimated Variability of Glucose (EVG). This data was collected from a type 1 diabetic's insulin pump and continuous glucose monitor (CGM). Unfortunately this data isnt entirely continous over the 2.5 year span and has gaps in some instances. This data has been exported directly from 30 day span .csv files containing 3 tables one for insulin dosage (bolus), bg, and EVG data. We split these files into the 3 respective tables and saved them to new tables containing the full time-span of data, these files are titles Bolus.csv, BG.csv, and EVG.csv. Here I will detail the contents of these files.\n",
    "\n",
    "1. Bolus\n",
    "    * Features:\n",
    "        * Type: Type of bolus event (Always 'Bolus').\n",
    "        * BolusType: Describes the wayin which the Bolus was used (categorical).\n",
    "        * BolusDeliveryMethod: Method used to deliver the bolus (Auto or Standard)(categorical).\n",
    "        * BG (mg/dL): Blood glucose levels at the time of bolus administration (continuous data).\n",
    "        * SerialNumber: Device identifier.\n",
    "        * CompletionDateTime: Timestamp when the dosing was completed.\n",
    "        * InsulinDelivered: The standard unit measure for the amount of insulin delivered (continuous data).\n",
    "        * FoodDelivered: Insulin delivered for food consumption (continuous data).\n",
    "        * CorrectionDelivered: Insulin delivered for BG correction (continuous data).\n",
    "        * CompletionStatusDesc: Status description of the dosage (categorical).\n",
    "        * BolexStartDateTime: Not Used In Export.\n",
    "        * BolexCompletionDateTime: Not Used In Export.\n",
    "        * BolexInsulinDelivered: Not Used In Export.\n",
    "        * BolexCompletionStatusDesc: Not Used In Export.\n",
    "        * StandardPercent: (Always 100).\n",
    "        * Duration (mins):(Always 0).\n",
    "        * CarbSize: The amount of carbohydrates consumed in grams (continuous data).\n",
    "        * TargetBG (mg/dL): Target blood glucose level for the subject in milligrams per deciliter (continuous data).\n",
    "        * CorrectionFactor: Insulin sensitivity factor (continuous data).\n",
    "        * CarbRatio: Insulin-to-carbohydrate ratio (continuous data).\n",
    "    * Notes:\n",
    "        * This table holds the most detailed records for insulin dosing decisions, food intake, and blood glucose corrections.\n",
    "        * This table will be very useful in modeling relationships between carbohydrate intake, insulin dosage, and BG levels.\n",
    "        * Bolus (Def: A large single doseage of insuline to lower a bloodsugar rise) refers to the device/method of insuline delivery. Which is automated through a pocket size device with a refillable tank of insulin.\n",
    "2. EVG Table\n",
    "    * Features:\n",
    "        * DeviceType: Type/Name of the device used to record the event.\n",
    "        * SerialNumber: Device identifier.\n",
    "        * Description: A text description of the type of data recorded (Always EVG).\n",
    "        * EventDateTime: Timestamp of when the measurement was recorded.\n",
    "        * Readings (mg/dL): The estimated glucose level at the recorded time, in milligrams per deciliter (mg/dL).\n",
    "    * Notes: \n",
    "        * The EVG data is collected directly from a CGM.\n",
    "        * EVG is designed for tracking overall trends and patterns in glucose levels rather than moment-to-moment decisions.\n",
    "        * Useful for identifying time-in-range, glucose variability, and predicting hypo/hyperglycemia over time.\n",
    "        \n",
    "3. BG Table\n",
    "    * Features:\n",
    "        * DeviceType: Type/Name of device used to measure blood glucose.\n",
    "        * SerialNumber: Device identifier.\n",
    "        * Description: A text description of the type of data recorded (Always BG)\n",
    "        * EventDateTime: Timestamp of the blood glucose measurement.\n",
    "        * BG (mg/dL): Blood glucose levels in milligrams per deciliter (continuous data).\n",
    "        * Note: Additional notes field (Always Blank).\n",
    "    * Notes:\n",
    "        * This data is manually entered and is recorded through a glucometer (Finger Prick).\n",
    "        * The BG measurement reflects the actual glucose levels at the time of measurement.\n",
    "\n",
    "\n",
    "### General Dataset Properties\n",
    "\n",
    "* Total \\# of Features:\n",
    "    * Bolus: 18 features\n",
    "    * EVG: 5 features\n",
    "    * BG: 6 features\n",
    "* \\# of Usable/Useful Features: (!!!!! Update possibly)\n",
    "    * Bolus: 12 features\n",
    "    * EVG: 2 features\n",
    "    * BG: 2 features\n",
    "* Table Sizes:\n",
    "    * Bolus: 11,348 Records\n",
    "    * EVG: 191,781 Records\n",
    "    * BG: 2,832 Records\n",
    "* Unique Dates: \n",
    "    * Bolus: 599 Days\n",
    "    * EVG: 666 Days\n",
    "    * BG: 663 Days\n",
    "* Intersecting Dates: 599\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Methodology for Data Preprocessing\n",
    "Given the nature of our timeseries problem after initial testing we identified some issues with the bolus data not being recorded at consistent intervals and only when administering insulin. To remedy this given that our EVG data is continous we decided to fill the gaps in our data with blank bolus records, only adding a 'Type' field equal to 'EVG', to denote the different types of records, and also adding the BG readings while setting the rest of the numeric fields to 0 and categorical fields to ''. Below I detail the preprocessing process.\n",
    "\n",
    "**1. Data Import and Initial Setup**\n",
    "* Loading Data:\n",
    "  * We decided to use the `EVG.csv` and `Bolus.csv` files as pandas DataFrames.\n",
    "  * In the function `convert_data(df):` we prepare the `EVG` data by mapping it to match the `Bolus` table structure for consistent formatting across datasets.\n",
    "* Merging Tables:\n",
    "  * We merged the `EVG` and `Bolus` tables into a single dataset using pandas' `concat` function filling the gaps in time.\n",
    "\n",
    "**2. Data Cleaning**\n",
    "* Datetime Conversion:\n",
    "  * Converted the `CompletionDateTime` column to the dataframe index.\n",
    "  * Rounded the `CompletionDateTime` to the nearest 5-minute interval to standardize timestamps.\n",
    "  * Identified and removed rows where the `CompletionDateTime` overlapped with `CGM` data to avoid redundancy or conflicting records.\n",
    "  * Dropped all data before 2024 to ensure continous data throughout the set.\n",
    "\n",
    "**3. Data Transformation**\n",
    "  * Applied one-hot encoding to categorical columns, such as `Type`, `BolusType`, `BolusDeliveryMethod`, and `CompletionStatusDesc`, to prepare them for machine learning models.\n",
    "    * We considered using encodings to map categorical values but resorted to one-hot encoding as it was more familiar to us.\n",
    "  * Removed columns that were not relevant or consistently populated (`SerialNumber`, `BolexStartDateTime`, `BolexCompletionDateTime`, etc.).\n",
    "  * Converted boolean columns to integer type for min-max scaling.\n",
    "  * Re-ordered columns and set the Date value as the Dataframe index.\n",
    "  * Ensured no missing values (`NaN`) remained in the dataset.\n",
    "  * Normalized the data using `MinMaxScaler` to normalize numeric fields to a range of [0, 1].\n",
    "\n",
    "\n",
    "### **Function Descriptions**\n",
    "\n",
    "**1. `round_to_nearest_5_minutes(df)`**\n",
    "* Purpose: To ensure all timestamps (`CompletionDateTime`) in the dataset are standardized by rounding them to the nearest 5-minute interval.\n",
    "* Steps:\n",
    "  1. Converts the `CompletionDateTime` column to datetime format.\n",
    "  2. Drops rows with invalid or missing datetime values.\n",
    "  3. Rounds valid `CompletionDateTime` values to the nearest 5-minute interval.\n",
    "\n",
    "\n",
    "**2. `remove_cgm_on_time_overlap(df)`**\n",
    "* Purpose: To remove redundant or conflicting data points from the dataset when multiple records share the same timestamp.\n",
    "* Steps:\n",
    "  1. Identifies duplicate `CompletionDateTime` entries across the dataset.\n",
    "  2. Filters out rows where the `Type` is `CGM` and the timestamp overlaps with other records.\n",
    "\n",
    "\n",
    "**3. `convert_data(df)`**\n",
    "* Purpose: To map the structure of the `EVG` table to match the `Bolus` table schema, enabling consistent formatting and integration.\n",
    "* Steps:\n",
    "  1. Creates a template `Bolus` table structure with predefined columns.\n",
    "  2. Renames relevant columns in the `EVG` data to align with the `Bolus` schema.\n",
    "  3. Adds missing columns to the mapped `EVG` table, using default values for both numeric and categorical fields.\n",
    "  4. Reorders the columns to match the `Bolus` table structure.\n",
    "\n",
    "\n",
    "**4. `onehot_encoding(df, columns)`**\n",
    "* Purpose: To transform specified categorical columns into one-hot encoded vectors.\n",
    "* Steps:\n",
    "  1. Uses pandas' `get_dummies()` method to create binary indicator columns for each category in the specified columns.\n",
    "  2. Appends these new columns to the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "def round_to_nearest_5_minutes(df):\n",
    "    # Ensure CompletionDateTime is in datetime format\n",
    "    df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'], errors='coerce')\n",
    "    # Drop rows with invalid datetime entries\n",
    "    df = df.dropna(subset=['CompletionDateTime'])\n",
    "    # Round CompletionDateTime to the nearest 5 minutes\n",
    "    df['CompletionDateTime'] = df['CompletionDateTime'].dt.round('5min')\n",
    "    return df\n",
    "\n",
    "def remove_cgm_on_time_overlap(df):\n",
    "    # Identify duplicates based on CompletionDateTime\n",
    "    duplicate_times = df[df.duplicated('CompletionDateTime', keep=False)]\n",
    "    # Filter out rows where Type is EVG and CompletionDateTime is duplicated\n",
    "    df = df[~((df['CompletionDateTime'].isin(duplicate_times['CompletionDateTime'])) & (df['Type'] == 'CGM'))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_data(df):\n",
    "    # Example Bolus table columns\n",
    "    bolus_columns = [\n",
    "        'Type', 'BolusType', 'BolusDeliveryMethod', 'BG (mg/dL)', 'SerialNumber',\n",
    "        'CompletionDateTime', 'InsulinDelivered', 'FoodDelivered', 'CorrectionDelivered',\n",
    "        'CompletionStatusDesc', 'BolexStartDateTime', 'BolexCompletionDateTime', \n",
    "        'BolexInsulinDelivered', 'BolexCompletionStatusDesc', 'StandardPercent', \n",
    "        'Duration (mins)', 'CarbSize', 'TargetBG (mg/dL)', 'CorrectionFactor', 'CarbRatio'\n",
    "    ]\n",
    "\n",
    "    # 1. Create a template DataFrame for Bolus table with default values\n",
    "    template_bolus = pd.DataFrame(columns=bolus_columns)\n",
    "\n",
    "    # Default values for missing fields\n",
    "    default_numeric = 0\n",
    "    default_categorical = ''\n",
    "\n",
    "    # 2. Map EVG fields to Bolus fields\n",
    "    mapped_bolus = evg_df.rename(columns={\n",
    "        'Readings (mg/dL)': 'BG (mg/dL)',\n",
    "        'SerialNumber': 'SerialNumber',\n",
    "        'EventDateTime': 'CompletionDateTime'\n",
    "    })\n",
    "\n",
    "    # 3. Add missing columns with default values\n",
    "    for col in bolus_columns:\n",
    "        if col not in mapped_bolus.columns:\n",
    "            if col in ['BG (mg/dL)', 'InsulinDelivered', 'FoodDelivered', 'CorrectionDelivered', \n",
    "                    'StandardPercent', 'Duration (mins)', 'CarbSize', 'TargetBG (mg/dL)', \n",
    "                    'CorrectionFactor', 'CarbRatio']:  # Numeric fields\n",
    "                mapped_bolus[col] = default_numeric\n",
    "            elif col == 'Type':\n",
    "                mapped_bolus[col] = 'CGM'\n",
    "            else:  # Categorical fields\n",
    "                mapped_bolus[col] = default_categorical\n",
    "\n",
    "    # 4. Reorder columns to match the Bolus table structure\n",
    "    mapped_bolus = mapped_bolus[bolus_columns]\n",
    "\n",
    "    # Resulting Bolus DataFrame\n",
    "    #print(mapped_bolus)\n",
    "    return mapped_bolus\n",
    "\n",
    "\n",
    "\n",
    "def onehot_encoding(df, columns):\n",
    "    df = pd.get_dummies(df, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "#file_path1 = './DataTables/BG.csv'  # Replace with the actual path for Table 1\n",
    "file_path2 = './DataTables/EVG.csv'  # Replace with the actual path for Table 2\n",
    "file_path3 = './DataTables/Bolus.csv'  # Replace with the actual path for Table 3\n",
    "\n",
    "# Load the data\n",
    "#df1 = pd.read_csv(file_path1)\n",
    "evg_df = pd.read_csv(file_path2)\n",
    "bolus_df = pd.read_csv(file_path3)\n",
    "\n",
    "evg_mapped = convert_data(evg_df)\n",
    "\n",
    "df = pd.concat([bolus_df, evg_mapped], ignore_index=True)\n",
    "#print(evg_mapped)\n",
    "\n",
    "# Round CompletionDateTime to the nearest 5 minutes\n",
    "df = round_to_nearest_5_minutes(df)\n",
    "\n",
    "# Sort the DataFrame by CompletionDateTime\n",
    "df = df.sort_values(by='CompletionDateTime').reset_index(drop=True)\n",
    "\n",
    "df = remove_cgm_on_time_overlap(df)\n",
    "columns=['Type', 'BolusType', 'BolusDeliveryMethod', 'CompletionStatusDesc']\n",
    "# Apply the function\n",
    "df = onehot_encoding(df, columns)\n",
    "columns=['SerialNumber', 'BolexStartDateTime', 'BolexCompletionDateTime', 'BolexInsulinDelivered', 'BolexCompletionStatusDesc']\n",
    "df = df.drop(columns=columns)\n",
    "bool_columns = df.select_dtypes(include='bool').columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "df.rename(columns={'CompletionDateTime': 'Date'}, inplace=True)\n",
    "\n",
    "# Move the 'Date' column to the first position\n",
    "first_col = df.pop('Date')  # Remove 'Date' column\n",
    "df.insert(0, 'Date', first_col)  # Reinsert it at position 0\n",
    "\n",
    "# Display the DataFrame\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "df.to_csv('test.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check for NaN values; there are none\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "# Organize data, make date into an index\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "# Note: we are not using any categorical data\n",
    "values = df.values\n",
    "# specify columns to plot\n",
    "#groups = [0, 1, 2, 3]\n",
    "#i = 1\n",
    "# plot each column\n",
    "#pyplot.figure()\n",
    "#for group in groups:\n",
    "#\tpyplot.subplot(len(groups), 1, i)\n",
    "#\tpyplot.plot(values[:, group])\n",
    "#\tpyplot.title(df.columns[group], y=0.5, loc='right')\n",
    "#\ti += 1\n",
    "#pyplot.show()\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Neural Networks to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks Improved: Long-Short Term Memory Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial by Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use the Time Series to Supervised Set formatter written by Jason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Variables From Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from our previously-made Bolus.csv table into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load Bolus Data\n",
    "file_path = './DataTables/Bolus.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'])\n",
    "df = df.sort_values(by='CompletionDateTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare and clean our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: False\n",
      "                      BG  InsulinDelivered  FoodDelivered  CarbSize\n",
      "Date                                                               \n",
      "2022-04-27 11:38:14    0              5.00           5.00        75\n",
      "2022-04-27 20:34:41  132              1.96           1.67        25\n",
      "2022-04-28 06:43:30    0              1.07           1.07        16\n",
      "2022-04-28 11:39:14    0              5.00           5.00        75\n",
      "2022-04-28 18:09:54    0              1.07           1.07        16\n"
     ]
    }
   ],
   "source": [
    "# Cull Featuers We're Not Using\n",
    "df = df[[\"BG (mg/dL)\", \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]]\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "\n",
    "# Organize data, make dataframe indexable by date\n",
    "df.columns = [\"BG\", \"Date\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "columns_titles = [\"Date\", \"BG\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "df = df.reindex(columns=columns_titles)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "# normalize features\n",
    "values = df.values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"n_in\" parameter in Jason's series_to_supervised() function allows us to transform the data such that each feature's time series is matched by *n* other time series of the feature which are offset by (t-1, t-2, ..., t-n) timesteps. If we leave it at default, we can then see, once we print the head of the dataframe, that it has output two timesteps for each variable. For variable 1, our blood glucose level as shown in the code cell above, there is var1(t-1) and var1(t). If you run the code cell below, you will notice that the time series var1(t-1) is the same series as var1(t) except that the entire series is offset one step into the past. This is the same for every one of the variables / features.\n",
    "\n",
    "It will be more convenient for us to do this later, but if we culled the last three timesteps, we would be left with a dataframe containing four of previous timestep series for all of our features (var1(t-1), var2(t-2), var3(t-3), var4(t-4)) — our input variables — and one timestep series for the current blood glucose level (var1(t)) — our output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var1(t)   var2(t)   var3(t)  \\\n",
      "1       0.00   0.267380   0.267380   0.166667     0.22  0.104813  0.089305   \n",
      "2       0.22   0.104813   0.089305   0.055556     0.00  0.057219  0.057219   \n",
      "3       0.00   0.057219   0.057219   0.035556     0.00  0.267380  0.267380   \n",
      "4       0.00   0.267380   0.267380   0.166667     0.00  0.057219  0.057219   \n",
      "5       0.00   0.057219   0.057219   0.035556     0.00  0.142781  0.142781   \n",
      "\n",
      "    var4(t)  \n",
      "1  0.055556  \n",
      "2  0.035556  \n",
      "3  0.166667  \n",
      "4  0.035556  \n",
      "5  0.088889  \n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with one timestep\n",
    "df_reframed = series_to_supervised(scaled, n_in=1, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set \"n_in\" parameter for more timesteps, the result is the same as above but with more timesteps. For our initial attempt, we will use 10 timesteps because we would later like to be able to use data from the last several hours as input into the LSTM to get our predicted blood-glucose level. \"n_out\" is set to 1 because we want to predict our current blood glucose level.\n",
    "\n",
    "We *would* like to be able to predict future blood glucose levels (which would be n_out >= 2), but because our data was not recorded at regular intervals, timestep predictions into the future would tell us \"how much?\" but not exactly \"when?\". If our data were recorded in regular one hour intervals, we could then rest assured that future timestep predictions would be *n* number of hours into the future precisely. This is a limitation in our dataset or, at least, in the tutorial we have followed to implement our LSTM. The tutorial was working with data on regular intervals. We are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-10)  var2(t-10)  var3(t-10)  var4(t-10)  var1(t-9)  var2(t-9)  \\\n",
      "10        0.00    0.267380    0.267380    0.166667       0.22   0.104813   \n",
      "11        0.22    0.104813    0.089305    0.055556       0.00   0.057219   \n",
      "12        0.00    0.057219    0.057219    0.035556       0.00   0.267380   \n",
      "13        0.00    0.267380    0.267380    0.166667       0.00   0.057219   \n",
      "14        0.00    0.057219    0.057219    0.035556       0.00   0.142781   \n",
      "\n",
      "    var3(t-9)  var4(t-9)  var1(t-8)  var2(t-8)  ...  var3(t-2)  var4(t-2)  \\\n",
      "10   0.089305   0.055556        0.0   0.057219  ...   0.246524   0.166667   \n",
      "11   0.057219   0.035556        0.0   0.267380  ...   0.000000   0.000000   \n",
      "12   0.267380   0.166667        0.0   0.057219  ...   0.106952   0.066667   \n",
      "13   0.057219   0.035556        0.0   0.142781  ...   0.000000   0.000000   \n",
      "14   0.142781   0.088889        0.0   0.057219  ...   0.000000   0.000000   \n",
      "\n",
      "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)   var2(t)   var3(t)  \\\n",
      "10   0.000000   0.032086   0.000000   0.000000  0.191667  0.106952  0.106952   \n",
      "11   0.191667   0.106952   0.106952   0.066667  0.000000  0.030481  0.000000   \n",
      "12   0.000000   0.030481   0.000000   0.000000  0.000000  0.051872  0.000000   \n",
      "13   0.000000   0.051872   0.000000   0.000000  0.470000  0.057219  0.057219   \n",
      "14   0.470000   0.057219   0.057219   0.035556  0.000000  0.008021  0.000000   \n",
      "\n",
      "     var4(t)  \n",
      "10  0.066667  \n",
      "11  0.000000  \n",
      "12  0.000000  \n",
      "13  0.035556  \n",
      "14  0.000000  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with multiple timesteps\n",
    "n_timesteps = 10\n",
    "df_reframed = series_to_supervised(scaled, n_in=n_timesteps, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Test and Train Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take data only from Fed 2024 through Oct 2024 because that time range in the Bolus dataset is the largest period of time for which there are not massive holes in the record. We split our data into a training set, comprised of the first half of our time range, and a testing set, comprised of the second half of our time range. We choose to split it this way rather than use standard shuffling methods of generating the sets because our data is only useful insofar as it records a time series. That is, if we scrambled the time series, the data would be relatively meaningless, yielding no info about the relative order nor trends in the data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "values = df_reframed.values\n",
    "index_2024 = 6893\n",
    "index_last = 11349\n",
    "index_midpoint = index_2024 + ((index_last - index_2024) // 2)\n",
    "train = values[index_2024:index_midpoint,:]\n",
    "test = values[index_midpoint:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cull all the current timestep features except variable 1, which is the blood glucose level variable we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2228, 10, 4)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 - 1s - 36ms/step - loss: 0.1840 - val_loss: 0.1288\n",
      "Epoch 2/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1288 - val_loss: 0.1245\n",
      "Epoch 3/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1270 - val_loss: 0.1236\n",
      "Epoch 4/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1263 - val_loss: 0.1229\n",
      "Epoch 5/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1258 - val_loss: 0.1223\n",
      "Epoch 6/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1253 - val_loss: 0.1219\n",
      "Epoch 7/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1250 - val_loss: 0.1215\n",
      "Epoch 8/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1245 - val_loss: 0.1211\n",
      "Epoch 9/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1243 - val_loss: 0.1206\n",
      "Epoch 10/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1240 - val_loss: 0.1202\n",
      "Epoch 11/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1236 - val_loss: 0.1198\n",
      "Epoch 12/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1234 - val_loss: 0.1194\n",
      "Epoch 13/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1229 - val_loss: 0.1190\n",
      "Epoch 14/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1228 - val_loss: 0.1188\n",
      "Epoch 15/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1225 - val_loss: 0.1185\n",
      "Epoch 16/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1223 - val_loss: 0.1182\n",
      "Epoch 17/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1222 - val_loss: 0.1179\n",
      "Epoch 18/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1220 - val_loss: 0.1178\n",
      "Epoch 19/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1218 - val_loss: 0.1176\n",
      "Epoch 20/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1217 - val_loss: 0.1175\n",
      "Epoch 21/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1215 - val_loss: 0.1173\n",
      "Epoch 22/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1213 - val_loss: 0.1171\n",
      "Epoch 23/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1213 - val_loss: 0.1171\n",
      "Epoch 24/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1211 - val_loss: 0.1170\n",
      "Epoch 25/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1211 - val_loss: 0.1170\n",
      "Epoch 26/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1210 - val_loss: 0.1170\n",
      "Epoch 27/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1209 - val_loss: 0.1169\n",
      "Epoch 28/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1209 - val_loss: 0.1169\n",
      "Epoch 29/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1208 - val_loss: 0.1170\n",
      "Epoch 30/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1208 - val_loss: 0.1167\n",
      "Epoch 31/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1207 - val_loss: 0.1168\n",
      "Epoch 32/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1207 - val_loss: 0.1168\n",
      "Epoch 33/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1206 - val_loss: 0.1168\n",
      "Epoch 34/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1206 - val_loss: 0.1167\n",
      "Epoch 35/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1205 - val_loss: 0.1166\n",
      "Epoch 36/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1205 - val_loss: 0.1169\n",
      "Epoch 37/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1204 - val_loss: 0.1166\n",
      "Epoch 38/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1204 - val_loss: 0.1167\n",
      "Epoch 39/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1203 - val_loss: 0.1169\n",
      "Epoch 40/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1169\n",
      "Epoch 41/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1203 - val_loss: 0.1169\n",
      "Epoch 42/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1171\n",
      "Epoch 43/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1169\n",
      "Epoch 44/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1167\n",
      "Epoch 45/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1170\n",
      "Epoch 46/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1168\n",
      "Epoch 47/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1167\n",
      "Epoch 48/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1166\n",
      "Epoch 49/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1169\n",
      "Epoch 50/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1168\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# design LSTM\n",
    "print(train_X.shape)\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# fit LSTM\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his original tutorial, Jason used RMSE as his measurement of the performance of the dataset. MAE is, however, significantly easier to understand intuitively. So, we wanted to use MAE, but RMSE is better at finding significant errors, which is especially important since we are trying to predict human blood glucose levels. Luckily, we can use both to gain a better understanding of our dataset.\n",
    "\n",
    "According to EUMeTrain.org, an international training project for the development of training material and training support in the field of satellite meteorology:\n",
    "\n",
    "\"The MAE and the RMSE can be used together to diagnose the variation in the errors in a set of forecasts. The RMSE will always be larger or equal to the MAE; the greater difference between them, the greater the variance in the individual errors in the sample. If the RMSE=MAE, then all the errors are of the same magnitude. Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.\"\n",
    "\n",
    "So in the code cell below, we calculated MAE, RMSE, and the resulting Variance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Test MAE: 8.372\n",
      "Test RMSE: 10.045\n",
      "Test Variance: 1.673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate MAE\n",
    "mae = np.sqrt(mean_absolute_error(inv_y, inv_yhat))\n",
    "print('Test MAE: %.3f' % mae)\n",
    "rmse = np.sqrt(root_mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "variance = rmse - mae\n",
    "print('Test Variance: %.3f' % variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code cell above results in an MAE of 8.372, an RMSE of 10.045, and a variance of 1.673. This variance is very low, and so it is unlikely that very large errors have occured. MAE tells us that the average difference between the forecasted blood glucose level and the observed was 8.3 mg/dL. The monitor our data came from records BG in a range from 0 to 400 mg/dL. 8.372 is only about 2.1% ((8.372 / 400) * 100 = 2.093). So, on average, our model's predicted BG level is only about 2.1% off of the actual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we overfitting? Underfitting? How do we tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which input variables from out dataset are the best predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the optimal number of timestamps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best LSTM hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tutorial: \"Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this)\"\n",
    "\n",
    "# Gird Search implementation like the one explained in Lecture (takes 15 mins to run currently)\n",
    "def GridSearchHyperparameters(num_neurons, num_epochs, batch_size, validation_data):\n",
    "    neurons_best = None\n",
    "    epochs_best = None\n",
    "    batch_size_best = None\n",
    "    mae_best = 1000000000000\n",
    "    for neurons in num_neurons:\n",
    "        for epochs in num_epochs:\n",
    "            for batches in batch_size:\n",
    "                print('------------------------------------')\n",
    "                print(\"neurons: \" + str(neurons), \"epochs: \" + str(epochs), \"batches: \" + str(batches))\n",
    "                # reset data\n",
    "                test_X = validation_data[0]\n",
    "                test_y = validation_data[1]\n",
    "                # design LSTM\n",
    "                model = Sequential()\n",
    "                model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "                model.add(LSTM(neurons))\n",
    "                model.add(Dense(1))\n",
    "                model.compile(loss='mae', optimizer='adam')\n",
    "                # fit LSTM\n",
    "                model.fit(train_X, train_y, epochs=epochs, batch_size=batches, validation_data=validation_data, verbose=0, shuffle=False)\n",
    "                # make a prediction\n",
    "                yhat = model.predict(test_X)\n",
    "                test_X = test_X.reshape((test_X.shape[0], n_timesteps*n_features))\n",
    "                # invert scaling for forecast\n",
    "                inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "                inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "                inv_yhat = inv_yhat[:,0]\n",
    "                # invert scaling for actual\n",
    "                test_y = test_y.reshape((len(test_y), 1))\n",
    "                inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "                inv_y = scaler.inverse_transform(inv_y)\n",
    "                inv_y = inv_y[:,0]\n",
    "                # calculate MAE\n",
    "                mae = np.sqrt(mean_absolute_error(inv_y, inv_yhat))\n",
    "                # best MAE decision\n",
    "                if mae < mae_best:\n",
    "                    neurons_best = neurons\n",
    "                    epochs_best = epochs\n",
    "                    batch_size_best = batches\n",
    "                    mae_best = mae\n",
    "    return mae_best, neurons_best, epochs_best, batch_size_best\n",
    "\n",
    "\n",
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))\n",
    "print(GridSearchHyperparameters(num_neurons=[5,10,25,50],\n",
    "                                num_epochs=[10, 20, 40, 80],\n",
    "                                batch_size=[10, 20, 40, 80],\n",
    "                                validation_data=(test_X, test_y))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "* Jason Brownlee's tutorial on how to create a multivariate LSTM in Python @ https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "* \"The AI Hacker\" on YouTube for his informative guide to RNNs @ https://youtu.be/LHXXI4-IEns?si=C4FoMP-Bv4xywsHZ\n",
    "* EUMeTrain.org for their explanation of MAE, RMSE, and how to assess variance between them @ https://resources.eumetrain.org/data/4/451/english/msg/ver_cont_var/uos3/uos3_ko1.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
