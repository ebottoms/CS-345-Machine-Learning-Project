{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Neural Networks to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks Improved: Long-Short Term Memory Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial by Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use the Time Series to Supervised Set formatter written by Jason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Variables From Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from our previously-made Bolus.csv table into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load Bolus Data\n",
    "file_path = './DataTables/Bolus.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'])\n",
    "df = df.sort_values(by='CompletionDateTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare and clean our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: False\n",
      "                      BG  InsulinDelivered  FoodDelivered  CarbSize\n",
      "Date                                                               \n",
      "2022-04-27 11:38:14    0              5.00           5.00        75\n",
      "2022-04-27 20:34:41  132              1.96           1.67        25\n",
      "2022-04-28 06:43:30    0              1.07           1.07        16\n",
      "2022-04-28 11:39:14    0              5.00           5.00        75\n",
      "2022-04-28 18:09:54    0              1.07           1.07        16\n"
     ]
    }
   ],
   "source": [
    "# Cull Featuers We're Not Using\n",
    "df = df[[\"BG (mg/dL)\", \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]]\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "\n",
    "# Organize data, make dataframe indexable by date\n",
    "df.columns = [\"BG\", \"Date\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "columns_titles = [\"Date\", \"BG\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "df = df.reindex(columns=columns_titles)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "# normalize features\n",
    "values = df.values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"n_in\" parameter in Jason's series_to_supervised() function allows us to transform the data such that each feature's time series is matched by *n* other time series of the feature which are offset by (t-1, t-2, ..., t-n) timesteps. If we leave it at default, we can then see, once we print the head of the dataframe, that it has output two timesteps for each variable. For variable 1, our blood glucose level as shown in the code cell above, there is var1(t-1) and var1(t). If you run the code cell below, you will notice that the time series var1(t-1) is the same series as var1(t) except that the entire series is offset one step into the past. This is the same for every one of the variables / features.\n",
    "\n",
    "It will be more convenient for us to do this later, but if we culled the last three timesteps, we would be left with a dataframe containing four of previous timestep series for all of our features (var1(t-1), var2(t-2), var3(t-3), var4(t-4)) — our input variables — and one timestep series for the current blood glucose level (var1(t)) — our output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var1(t)   var2(t)   var3(t)  \\\n",
      "1       0.00   0.267380   0.267380   0.166667     0.22  0.104813  0.089305   \n",
      "2       0.22   0.104813   0.089305   0.055556     0.00  0.057219  0.057219   \n",
      "3       0.00   0.057219   0.057219   0.035556     0.00  0.267380  0.267380   \n",
      "4       0.00   0.267380   0.267380   0.166667     0.00  0.057219  0.057219   \n",
      "5       0.00   0.057219   0.057219   0.035556     0.00  0.142781  0.142781   \n",
      "\n",
      "    var4(t)  \n",
      "1  0.055556  \n",
      "2  0.035556  \n",
      "3  0.166667  \n",
      "4  0.035556  \n",
      "5  0.088889  \n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with one timestep\n",
    "df_reframed = series_to_supervised(scaled, n_in=1, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set \"n_in\" parameter for more timesteps, the result is the same as above but with more timesteps. For our initial attempt, we will use 10 timesteps because we would later like to be able to use data from the last several hours as input into the LSTM to get our predicted blood-glucose level. \"n_out\" is set to 1 because we want to predict our current blood glucose level.\n",
    "\n",
    "We *would* like to be able to predict future blood glucose levels (which would be n_out >= 2), but because our data was not recorded at regular intervals, timestep predictions into the future would tell us \"how much?\" but not exactly \"when?\". If our data were recorded in regular one hour intervals, we could then rest assured that future timestep predictions would be *n* number of hours into the future precisely. This is a limitation in our dataset or, at least, in the tutorial we have followed to implement our LSTM. The tutorial was working with data on regular intervals. We are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-10)  var2(t-10)  var3(t-10)  var4(t-10)  var1(t-9)  var2(t-9)  \\\n",
      "10        0.00    0.267380    0.267380    0.166667       0.22   0.104813   \n",
      "11        0.22    0.104813    0.089305    0.055556       0.00   0.057219   \n",
      "12        0.00    0.057219    0.057219    0.035556       0.00   0.267380   \n",
      "13        0.00    0.267380    0.267380    0.166667       0.00   0.057219   \n",
      "14        0.00    0.057219    0.057219    0.035556       0.00   0.142781   \n",
      "\n",
      "    var3(t-9)  var4(t-9)  var1(t-8)  var2(t-8)  ...  var3(t-2)  var4(t-2)  \\\n",
      "10   0.089305   0.055556        0.0   0.057219  ...   0.246524   0.166667   \n",
      "11   0.057219   0.035556        0.0   0.267380  ...   0.000000   0.000000   \n",
      "12   0.267380   0.166667        0.0   0.057219  ...   0.106952   0.066667   \n",
      "13   0.057219   0.035556        0.0   0.142781  ...   0.000000   0.000000   \n",
      "14   0.142781   0.088889        0.0   0.057219  ...   0.000000   0.000000   \n",
      "\n",
      "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)   var2(t)   var3(t)  \\\n",
      "10   0.000000   0.032086   0.000000   0.000000  0.191667  0.106952  0.106952   \n",
      "11   0.191667   0.106952   0.106952   0.066667  0.000000  0.030481  0.000000   \n",
      "12   0.000000   0.030481   0.000000   0.000000  0.000000  0.051872  0.000000   \n",
      "13   0.000000   0.051872   0.000000   0.000000  0.470000  0.057219  0.057219   \n",
      "14   0.470000   0.057219   0.057219   0.035556  0.000000  0.008021  0.000000   \n",
      "\n",
      "     var4(t)  \n",
      "10  0.066667  \n",
      "11  0.000000  \n",
      "12  0.000000  \n",
      "13  0.035556  \n",
      "14  0.000000  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with multiple timesteps\n",
    "n_timesteps = 10\n",
    "df_reframed = series_to_supervised(scaled, n_in=n_timesteps, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Test and Train Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take data only from Fed 2024 through Oct 2024 because that time range in the Bolus dataset is the largest period of time for which there are not massive holes in the record. We split our data into a training set, comprised of the first half of our time range, and a testing set, comprised of the second half of our time range. We choose to split it this way rather than use standard shuffling methods of generating the sets because our data is only useful insofar as it records a time series. That is, if we scrambled the time series, the data would be relatively meaningless, yielding no info about the relative order nor trends in the data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "values = df_reframed.values\n",
    "index_2024 = 6893\n",
    "index_last = 11349\n",
    "index_midpoint = index_2024 + ((index_last - index_2024) // 2)\n",
    "train = values[index_2024:index_midpoint,:]\n",
    "test = values[index_midpoint:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cull all the current timestep features except variable 1, which is the blood glucose level variable we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2228, 10, 4)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 - 1s - 36ms/step - loss: 0.1840 - val_loss: 0.1288\n",
      "Epoch 2/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1288 - val_loss: 0.1245\n",
      "Epoch 3/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1270 - val_loss: 0.1236\n",
      "Epoch 4/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1263 - val_loss: 0.1229\n",
      "Epoch 5/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1258 - val_loss: 0.1223\n",
      "Epoch 6/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1253 - val_loss: 0.1219\n",
      "Epoch 7/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1250 - val_loss: 0.1215\n",
      "Epoch 8/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1245 - val_loss: 0.1211\n",
      "Epoch 9/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1243 - val_loss: 0.1206\n",
      "Epoch 10/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1240 - val_loss: 0.1202\n",
      "Epoch 11/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1236 - val_loss: 0.1198\n",
      "Epoch 12/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1234 - val_loss: 0.1194\n",
      "Epoch 13/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1229 - val_loss: 0.1190\n",
      "Epoch 14/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1228 - val_loss: 0.1188\n",
      "Epoch 15/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1225 - val_loss: 0.1185\n",
      "Epoch 16/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1223 - val_loss: 0.1182\n",
      "Epoch 17/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1222 - val_loss: 0.1179\n",
      "Epoch 18/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1220 - val_loss: 0.1178\n",
      "Epoch 19/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1218 - val_loss: 0.1176\n",
      "Epoch 20/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1217 - val_loss: 0.1175\n",
      "Epoch 21/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1215 - val_loss: 0.1173\n",
      "Epoch 22/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1213 - val_loss: 0.1171\n",
      "Epoch 23/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1213 - val_loss: 0.1171\n",
      "Epoch 24/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1211 - val_loss: 0.1170\n",
      "Epoch 25/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1211 - val_loss: 0.1170\n",
      "Epoch 26/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1210 - val_loss: 0.1170\n",
      "Epoch 27/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1209 - val_loss: 0.1169\n",
      "Epoch 28/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1209 - val_loss: 0.1169\n",
      "Epoch 29/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1208 - val_loss: 0.1170\n",
      "Epoch 30/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1208 - val_loss: 0.1167\n",
      "Epoch 31/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1207 - val_loss: 0.1168\n",
      "Epoch 32/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1207 - val_loss: 0.1168\n",
      "Epoch 33/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1206 - val_loss: 0.1168\n",
      "Epoch 34/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1206 - val_loss: 0.1167\n",
      "Epoch 35/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1205 - val_loss: 0.1166\n",
      "Epoch 36/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1205 - val_loss: 0.1169\n",
      "Epoch 37/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1204 - val_loss: 0.1166\n",
      "Epoch 38/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1204 - val_loss: 0.1167\n",
      "Epoch 39/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1203 - val_loss: 0.1169\n",
      "Epoch 40/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1169\n",
      "Epoch 41/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1203 - val_loss: 0.1169\n",
      "Epoch 42/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1171\n",
      "Epoch 43/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1202 - val_loss: 0.1169\n",
      "Epoch 44/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1167\n",
      "Epoch 45/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1170\n",
      "Epoch 46/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1168\n",
      "Epoch 47/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1201 - val_loss: 0.1167\n",
      "Epoch 48/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1166\n",
      "Epoch 49/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1169\n",
      "Epoch 50/50\n",
      "31/31 - 0s - 4ms/step - loss: 0.1200 - val_loss: 0.1168\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# design LSTM\n",
    "print(train_X.shape)\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# fit LSTM\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his original tutorial, Jason used RMSE as his measurement of the performance of the dataset. MAE is, however, significantly easier to understand intuitively. So, we wanted to use MAE, but RMSE is better at finding significant errors, which is especially important since we are trying to predict human blood glucose levels. Luckily, we can use both to gain a better understanding of our dataset.\n",
    "\n",
    "According to EUMeTrain.org, an international training project for the development of training material and training support in the field of satellite meteorology:\n",
    "\n",
    "\"The MAE and the RMSE can be used together to diagnose the variation in the errors in a set of forecasts. The RMSE will always be larger or equal to the MAE; the greater difference between them, the greater the variance in the individual errors in the sample. If the RMSE=MAE, then all the errors are of the same magnitude. Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.\"\n",
    "\n",
    "So in the code cell below, we calculated MAE, RMSE, and the resulting Variance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Test MAE: 8.372\n",
      "Test RMSE: 10.045\n",
      "Test Variance: 1.673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate MAE\n",
    "mae = np.sqrt(mean_absolute_error(inv_y, inv_yhat))\n",
    "print('Test MAE: %.3f' % mae)\n",
    "rmse = np.sqrt(root_mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "variance = rmse - mae\n",
    "print('Test Variance: %.3f' % variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code cell above results in an MAE of 8.372, an RMSE of 10.045, and a variance of 1.673. This variance is very low, and so it is unlikely that very large errors have occured. MAE tells us that the average difference between the forecasted blood glucose level and the observed was 8.3 mg/dL. The monitor our data came from records BG in a range from 0 to 400 mg/dL. 8.372 is only about 2.1% ((8.372 / 400) * 100 = 2.093). So, on average, our model's predicted BG level is only about 2.1% off of the actual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we overfitting? Underfitting? How do we tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which input variables from out dataset are the best predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the optimal number of timestamps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best LSTM hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tutorial: \"Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this)\"\n",
    "\n",
    "# Gird Search implementation like the one explained in Lecture (takes 15 mins to run currently)\n",
    "def GridSearchHyperparameters(num_neurons, num_epochs, batch_size, validation_data):\n",
    "    neurons_best = None\n",
    "    epochs_best = None\n",
    "    batch_size_best = None\n",
    "    mae_best = 1000000000000\n",
    "    for neurons in num_neurons:\n",
    "        for epochs in num_epochs:\n",
    "            for batches in batch_size:\n",
    "                print('------------------------------------')\n",
    "                print(\"neurons: \" + str(neurons), \"epochs: \" + str(epochs), \"batches: \" + str(batches))\n",
    "                # reset data\n",
    "                test_X = validation_data[0]\n",
    "                test_y = validation_data[1]\n",
    "                # design LSTM\n",
    "                model = Sequential()\n",
    "                model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "                model.add(LSTM(neurons))\n",
    "                model.add(Dense(1))\n",
    "                model.compile(loss='mae', optimizer='adam')\n",
    "                # fit LSTM\n",
    "                model.fit(train_X, train_y, epochs=epochs, batch_size=batches, validation_data=validation_data, verbose=0, shuffle=False)\n",
    "                # make a prediction\n",
    "                yhat = model.predict(test_X)\n",
    "                test_X = test_X.reshape((test_X.shape[0], n_timesteps*n_features))\n",
    "                # invert scaling for forecast\n",
    "                inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "                inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "                inv_yhat = inv_yhat[:,0]\n",
    "                # invert scaling for actual\n",
    "                test_y = test_y.reshape((len(test_y), 1))\n",
    "                inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "                inv_y = scaler.inverse_transform(inv_y)\n",
    "                inv_y = inv_y[:,0]\n",
    "                # calculate MAE\n",
    "                mae = np.sqrt(mean_absolute_error(inv_y, inv_yhat))\n",
    "                # best MAE decision\n",
    "                if mae < mae_best:\n",
    "                    neurons_best = neurons\n",
    "                    epochs_best = epochs\n",
    "                    batch_size_best = batches\n",
    "                    mae_best = mae\n",
    "    return mae_best, neurons_best, epochs_best, batch_size_best\n",
    "\n",
    "\n",
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))\n",
    "print(GridSearchHyperparameters(num_neurons=[5,10,25,50],\n",
    "                                num_epochs=[10, 20, 40, 80],\n",
    "                                batch_size=[10, 20, 40, 80],\n",
    "                                validation_data=(test_X, test_y))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "* Jason Brownlee's tutorial on how to create a multivariate LSTM in Python @ https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "* \"The AI Hacker\" on YouTube for his informative guide to RNNs @ https://youtu.be/LHXXI4-IEns?si=C4FoMP-Bv4xywsHZ\n",
    "* EUMeTrain.org for their explanation of MAE, RMSE, and how to assess variance between them @ https://resources.eumetrain.org/data/4/451/english/msg/ver_cont_var/uos3/uos3_ko1.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
