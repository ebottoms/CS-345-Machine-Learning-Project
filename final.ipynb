{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Neural Networks to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks Improved: Long-Short Term Memory Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial by Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use the Time Series to Supervised Set formatter written by Jason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Variables From Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from our previously-made Bolus.csv table into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load Bolus Data\n",
    "file_path = './DataTables/Bolus.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'])\n",
    "df = df.sort_values(by='CompletionDateTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare and clean our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: False\n",
      "                      BG  InsulinDelivered  FoodDelivered  CarbSize\n",
      "Date                                                               \n",
      "2022-04-27 11:38:14    0              5.00           5.00        75\n",
      "2022-04-27 20:34:41  132              1.96           1.67        25\n",
      "2022-04-28 06:43:30    0              1.07           1.07        16\n",
      "2022-04-28 11:39:14    0              5.00           5.00        75\n",
      "2022-04-28 18:09:54    0              1.07           1.07        16\n"
     ]
    }
   ],
   "source": [
    "# Cull Featuers We're Not Using\n",
    "df = df[[\"BG (mg/dL)\", \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]]\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "\n",
    "# Organize data, make dataframe indexable by date\n",
    "df.columns = [\"BG\", \"Date\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "columns_titles = [\"Date\", \"BG\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "df = df.reindex(columns=columns_titles)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "# normalize features\n",
    "values = df.values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"n_in\" parameter in Jason's series_to_supervised() function allows us to transform the data such that each feature's time series is matched by *n* other time series of the feature which are offset by (t-1, t-2, ..., t-n) timesteps. If we leave it at default, we can then see, once we print the head of the dataframe, that it has output two timesteps for each variable. For variable 1, our blood glucose level as shown in the code cell above, there is var1(t-1) and var1(t). If you run the code cell below, you will notice that the time series var1(t-1) is the same series as var1(t) except that the entire series is offset one step into the past. This is the same for every one of the variables / features.\n",
    "\n",
    "It will be more convenient for us to do this later, but if we culled the last three timesteps, we would be left with a dataframe containing four of previous timestep series for all of our features (var1(t-1), var2(t-2), var3(t-3), var4(t-4)) — our input variables — and one timestep series for the current blood glucose level (var1(t)) — our output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var1(t)   var2(t)   var3(t)  \\\n",
      "1       0.00   0.267380   0.267380   0.166667     0.22  0.104813  0.089305   \n",
      "2       0.22   0.104813   0.089305   0.055556     0.00  0.057219  0.057219   \n",
      "3       0.00   0.057219   0.057219   0.035556     0.00  0.267380  0.267380   \n",
      "4       0.00   0.267380   0.267380   0.166667     0.00  0.057219  0.057219   \n",
      "5       0.00   0.057219   0.057219   0.035556     0.00  0.142781  0.142781   \n",
      "\n",
      "    var4(t)  \n",
      "1  0.055556  \n",
      "2  0.035556  \n",
      "3  0.166667  \n",
      "4  0.035556  \n",
      "5  0.088889  \n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with one timestep\n",
    "df_reframed = series_to_supervised(scaled, n_in=1, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set \"n_in\" parameter for more timesteps, the result is the same as above but with more timesteps. For our initial attempt, we will use 10 timesteps because we would later like to be able to use data from the last several hours as input into the LSTM to get our predicted blood-glucose level. \"n_out\" is set to 1 because we want to predict our current blood glucose level.\n",
    "\n",
    "We *would* like to be able to predict future blood glucose levels (which would be n_out >= 2), but because our data was not recorded at regular intervals, timestep predictions into the future would tell us \"how much?\" but not exactly \"when?\". If our data were recorded in regular one hour intervals, we could then rest assured that future timestep predictions would be *n* number of hours into the future precisely. This is a limitation in our dataset or, at least, in the tutorial we have followed to implement our LSTM. The tutorial was working with data on regular intervals. We are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-10)  var2(t-10)  var3(t-10)  var4(t-10)  var1(t-9)  var2(t-9)  \\\n",
      "10        0.00    0.267380    0.267380    0.166667       0.22   0.104813   \n",
      "11        0.22    0.104813    0.089305    0.055556       0.00   0.057219   \n",
      "12        0.00    0.057219    0.057219    0.035556       0.00   0.267380   \n",
      "13        0.00    0.267380    0.267380    0.166667       0.00   0.057219   \n",
      "14        0.00    0.057219    0.057219    0.035556       0.00   0.142781   \n",
      "\n",
      "    var3(t-9)  var4(t-9)  var1(t-8)  var2(t-8)  ...  var3(t-2)  var4(t-2)  \\\n",
      "10   0.089305   0.055556        0.0   0.057219  ...   0.246524   0.166667   \n",
      "11   0.057219   0.035556        0.0   0.267380  ...   0.000000   0.000000   \n",
      "12   0.267380   0.166667        0.0   0.057219  ...   0.106952   0.066667   \n",
      "13   0.057219   0.035556        0.0   0.142781  ...   0.000000   0.000000   \n",
      "14   0.142781   0.088889        0.0   0.057219  ...   0.000000   0.000000   \n",
      "\n",
      "    var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)   var2(t)   var3(t)  \\\n",
      "10   0.000000   0.032086   0.000000   0.000000  0.191667  0.106952  0.106952   \n",
      "11   0.191667   0.106952   0.106952   0.066667  0.000000  0.030481  0.000000   \n",
      "12   0.000000   0.030481   0.000000   0.000000  0.000000  0.051872  0.000000   \n",
      "13   0.000000   0.051872   0.000000   0.000000  0.470000  0.057219  0.057219   \n",
      "14   0.470000   0.057219   0.057219   0.035556  0.000000  0.008021  0.000000   \n",
      "\n",
      "     var4(t)  \n",
      "10  0.066667  \n",
      "11  0.000000  \n",
      "12  0.000000  \n",
      "13  0.035556  \n",
      "14  0.000000  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform to supervised set with multiple timesteps\n",
    "n_timesteps = 10\n",
    "df_reframed = series_to_supervised(scaled, n_in=n_timesteps, n_out=1)\n",
    "print(df_reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Test and Train Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take data only from Fed 2024 through Oct 2024 because that time range in the Bolus dataset is the largest period of time for which there are not massive holes in the record. We split our data into a training set, comprised of the first half of our time range, and a testing set, comprised of the second half of our time range. We choose to split it this way rather than use standard shuffling methods of generating the sets because our data is only useful insofar as it records a time series. That is, if we scrambled the time series, the data would be relatively meaningless, yielding no info about the relative order nor trends in the data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "values = df_reframed.values\n",
    "index_2024 = 6893\n",
    "index_last = 11349\n",
    "index_midpoint = index_2024 + ((index_last - index_2024) // 2)\n",
    "train = values[index_2024:index_midpoint,:]\n",
    "test = values[index_midpoint:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cull all the current timestep features except variable 1, which is the blood glucose level variable we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# design LSTM\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m100\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(train_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], train_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# design LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# fit LSTM\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we overfitting? Underfitting? How do we tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which input variables from out dataset are the best predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the optimal number of timestamps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best LSTM hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
