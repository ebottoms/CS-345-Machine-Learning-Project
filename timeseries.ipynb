{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Notes\n",
    "* Not all Days are covered, at points there are significant (5-8 week) gaps in data.\n",
    "* Different Devices used to collect data may overlap times (device marked as 'UNKNOWN' in data)\n",
    "* 3 different tables (I think our main focus should be on the third table that has the pump data coupled with the best of the BG tables (1 or 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Table 1 DataFrame:\n",
      "  DeviceType SerialNumber Description        EventDateTime Readings (mg/dL)\n",
      "0    Unknown       870772         EGV  2023-03-01T00:00:55              174\n",
      "1    Unknown       870772         EGV  2023-03-01T00:05:55              172\n",
      "2    Unknown       870772         EGV  2023-03-01T00:10:55              171\n",
      "3    Unknown       870772         EGV  2023-03-01T00:15:55              168\n",
      "4    Unknown       870772         EGV  2023-03-01T00:20:55              165\n",
      "\n",
      "Table 2 DataFrame:\n",
      "  DeviceType SerialNumber Description        EventDateTime BG (mg/dL) Note\n",
      "0    Unknown       870772          BG  2023-03-01T16:07:59        238     \n",
      "1    Unknown       870772          BG  2023-03-01T18:04:52        382     \n",
      "2    Unknown       870772          BG  2023-03-01T20:09:14        156     \n",
      "3    Unknown       870772          BG  2023-03-01T20:31:14        248     \n",
      "4    Unknown       870772          BG  2023-03-02T12:46:15        157     \n",
      "\n",
      "Table 3 DataFrame:\n",
      "    Type BolusType BolusDeliveryMethod BG (mg/dL) SerialNumber  \\\n",
      "0  Bolus      Auto                Auto          0       870772   \n",
      "1  Bolus      Auto                Auto          0       870772   \n",
      "2  Bolus      Auto                Auto          0       870772   \n",
      "3  Bolus      Auto                Auto          0       870772   \n",
      "4  Bolus      Auto                Auto          0       870772   \n",
      "\n",
      "    CompletionDateTime InsulinDelivered FoodDelivered CorrectionDelivered  \\\n",
      "0  2023-03-01T01:02:24             0.65             0                   0   \n",
      "1  2023-03-01T02:12:13             1.24             0                   0   \n",
      "2  2023-03-01T03:22:44             0.64             0                   0   \n",
      "3  2023-03-01T07:48:06             0.62             0                   0   \n",
      "4  2023-03-01T08:53:18             1.01             0                   0   \n",
      "\n",
      "  CompletionStatusDesc BolexStartDateTime BolexCompletionDateTime  \\\n",
      "0            Completed                                              \n",
      "1            Completed                                              \n",
      "2            Completed                                              \n",
      "3            Completed                                              \n",
      "4            Completed                                              \n",
      "\n",
      "  BolexInsulinDelivered BolexCompletionStatusDesc StandardPercent  \\\n",
      "0                                                             100   \n",
      "1                                                             100   \n",
      "2                                                             100   \n",
      "3                                                             100   \n",
      "4                                                             100   \n",
      "\n",
      "  Duration (mins) CarbSize TargetBG (mg/dL) CorrectionFactor CarbRatio  \n",
      "0               0        0              110               65       0.0  \n",
      "1               0        0              110               65       0.0  \n",
      "2               0        0              110               65       0.0  \n",
      "3               0        0              110               65       0.0  \n",
      "4               0        0              110               65       0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def load_export_csv(file_path):\n",
    "    # Define the headers for each table to identify them\n",
    "    table_headers = {\n",
    "        'table1': [\"DeviceType\", \"SerialNumber\", \"Description\", \"EventDateTime\", \"Readings (mg/dL)\"],\n",
    "        'table2': [\"DeviceType\", \"SerialNumber\", \"Description\", \"EventDateTime\", \"BG (mg/dL)\", \"Note\"],\n",
    "        'table3': [\"Type\", \"BolusType\", \"BolusDeliveryMethod\", \"BG (mg/dL)\", \"SerialNumber\",\n",
    "                   \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CorrectionDelivered\",\n",
    "                   \"CompletionStatusDesc\", \"BolexStartDateTime\", \"BolexCompletionDateTime\",\n",
    "                   \"BolexInsulinDelivered\", \"BolexCompletionStatusDesc\", \"StandardPercent\",\n",
    "                   \"Duration (mins)\", \"CarbSize\", \"TargetBG (mg/dL)\", \"CorrectionFactor\",\n",
    "                   \"CarbRatio\"]\n",
    "    }\n",
    "    \n",
    "    # Initialize data storage for each table\n",
    "    data_tables = {\n",
    "        'table1': [],\n",
    "        'table2': [],\n",
    "        'table3': []\n",
    "    }\n",
    "    \n",
    "    current_table = None  # To keep track of which table we're currently reading\n",
    "    line_number = 0  # To track line numbers for debugging\n",
    "    \n",
    "    with open(file_path, 'r', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            # Strip whitespace from each cell\n",
    "            row = [cell.strip() for cell in row]\n",
    "            \n",
    "            # Debugging: Print current row and line number\n",
    "            # print(f\"Line {line_number}: {row}\")\n",
    "            \n",
    "            # Check if the row matches any table header\n",
    "            if row[:len(table_headers['table1'])] == table_headers['table1']:\n",
    "                current_table = 'table1'\n",
    "                # print(f\"Detected header for table1 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif row[:len(table_headers['table2'])] == table_headers['table2']:\n",
    "                current_table = 'table2'\n",
    "                # print(f\"Detected header for table2 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif row[:len(table_headers['table3'])] == table_headers['table3']:\n",
    "                current_table = 'table3'\n",
    "                # print(f\"Detected header for table3 at line {line_number}\")\n",
    "                continue  # Skip the header row\n",
    "            elif not any(cell for cell in row):\n",
    "                # Empty row signifies possible separation; skip\n",
    "                current_table = None\n",
    "                # print(f\"Detected empty row at line {line_number}; resetting current_table\")\n",
    "                continue\n",
    "            \n",
    "            # If current_table is set, append the row to the corresponding data list\n",
    "            if current_table:\n",
    "                expected_columns = len(table_headers[current_table])\n",
    "                actual_columns = len(row)\n",
    "                \n",
    "                if actual_columns != expected_columns:\n",
    "                    print(f\"Warning: Line {line_number} has {actual_columns} columns, expected {expected_columns}. Skipping row.\")\n",
    "                    continue  # Skip rows that don't match the expected column count\n",
    "                \n",
    "                # Replace '(Data)' placeholders with actual data if necessary\n",
    "                cleaned_row = [cell if cell != '(Data)' else None for cell in row]\n",
    "                data_tables[current_table].append(cleaned_row)\n",
    "            else:\n",
    "                # Rows outside of any table headers are ignored\n",
    "                print(f\"Warning: Line {line_number} is outside of any table. Skipping row.\")\n",
    "                continue\n",
    "    \n",
    "    # Convert lists to DataFrames with appropriate columns\n",
    "    df_tables = {}\n",
    "    for table_key, data in data_tables.items():\n",
    "        if data:  # Only create DataFrame if there's data\n",
    "            df = pd.DataFrame(data, columns=table_headers[table_key])\n",
    "            df_tables[table_key] = df\n",
    "        else:\n",
    "            df_tables[table_key] = pd.DataFrame(columns=table_headers[table_key])\n",
    "    \n",
    "    return df_tables['table1'], df_tables['table2'], df_tables['table3']\n",
    "\n",
    "\n",
    "\n",
    "file_path = './Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920-2.csv'  \n",
    "df_table1, df_table2, df_table3 = load_export_csv(file_path)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Table 1 DataFrame:\")\n",
    "print(df_table1.head())\n",
    "\n",
    "print(\"\\nTable 2 DataFrame:\")\n",
    "print(df_table2.head())\n",
    "\n",
    "print(\"\\nTable 3 DataFrame:\")\n",
    "print(df_table3.head())\n",
    "\n",
    "# Optionally, save the DataFrames to separate CSV files\n",
    "df_table1.to_csv(os.path.join('./DataTables', 'table_1.csv'), index=False)\n",
    "df_table2.to_csv(os.path.join('./DataTables', 'table_2.csv'), index=False)\n",
    "df_table3.to_csv(os.path.join('./DataTables', 'table_3.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/2022PumpData\n",
      "./Data/2023PumpData\n",
      "./Data/2024PumpData\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1916.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1917-2.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1917.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1910.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1908.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1916-2.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1918.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1912.csv\n",
      "./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1914.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924-3.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1923.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1922-2.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920-2.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921-2.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921-3.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1922.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1923-2.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1918.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924-2.csv\n",
      "./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1900-2.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1858-2.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1859.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901-3.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1857.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1858.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1859-2.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1900.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901.csv\n",
      "./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901-2.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def gather_csv_files(main_directory):\n",
    "    \"\"\"\n",
    "    Gathers all CSV files from subdirectories (2022PumpData, 2023PumpData, 2024PumpData)\n",
    "    in the 'data' directory located in the main directory.\n",
    "\n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of paths to all CSV files found in the specified subdirectories.\n",
    "    \"\"\"\n",
    "    # Define the parent directory\n",
    "    parent_directory = os.path.join(main_directory, 'Data')\n",
    "    \n",
    "    # List of subdirectories to search\n",
    "    subdirectories = ['2022PumpData', '2023PumpData', '2024PumpData']\n",
    "    \n",
    "    # Collect CSV files from all subdirectories\n",
    "    csv_files = []\n",
    "    for subdir in subdirectories:\n",
    "        subdir_path = os.path.join(parent_directory, subdir)\n",
    "        print(subdir_path)\n",
    "        csv_files.extend(glob.glob(os.path.join(subdir_path, '*.csv')))\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# Usage example\n",
    "main_directory = './'  # Replace with the path to your main directory\n",
    "csv_files = gather_csv_files(main_directory)\n",
    "\n",
    "# Print the gathered CSV file paths\n",
    "for file in csv_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/2022PumpData\n",
      "./Data/2023PumpData\n",
      "./Data/2024PumpData\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1916.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1917-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1917.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1910.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1908.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1916-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1918.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1912.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2022PumpData/CSV_redacted_90945369_02Dec2024_1914.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924-3.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1923.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1922-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921-3.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1922.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1923-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1918.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1920.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1924-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2023PumpData/CSV_redacted_90945369_02Dec2024_1921.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1900-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1858-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1859.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901-3.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1857.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1858.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1859-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1900.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Processing file: ./Data/2024PumpData/CSV_redacted_90945369_02Dec2024_1901-2.csv\n",
      "Warning: Line 1 is outside of any table. Skipping row.\n",
      "Warning: Line 2 is outside of any table. Skipping row.\n",
      "Warning: Line 3 is outside of any table. Skipping row.\n",
      "Warning: Line 4 is outside of any table. Skipping row.\n",
      "Warning: Line 5 is outside of any table. Skipping row.\n",
      "Combined tables saved to: ./DataTables\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_all_csv_files_combined(main_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the 'Data/2022PumpData', 'Data/2023PumpData', and 'Data/2024PumpData'\n",
    "    subdirectories, extracting tables and appending them into combined CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory containing the 'Data' folder.\n",
    "        output_directory (str): The path to the directory where combined tables will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Gather all CSV files\n",
    "    csv_files = gather_csv_files(main_directory)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Initialize empty DataFrames for combined output\n",
    "    combined_table1 = pd.DataFrame()\n",
    "    combined_table2 = pd.DataFrame()\n",
    "    combined_table3 = pd.DataFrame()\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file and extract tables\n",
    "            df_table1, df_table2, df_table3 = load_export_csv(file_path)\n",
    "            \n",
    "            # Append each table to its respective combined DataFrame\n",
    "            combined_table1 = pd.concat([combined_table1, df_table1], ignore_index=True)\n",
    "            combined_table2 = pd.concat([combined_table2, df_table2], ignore_index=True)\n",
    "            combined_table3 = pd.concat([combined_table3, df_table3], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Define output file paths\n",
    "    table1_file = os.path.join(output_directory, 'EVG.csv')\n",
    "    table2_file = os.path.join(output_directory, 'BG.csv')\n",
    "    table3_file = os.path.join(output_directory, 'Bolus.csv')\n",
    "\n",
    "    # Save the combined tables to CSV\n",
    "    combined_table1.to_csv(table1_file, index=False)\n",
    "    combined_table2.to_csv(table2_file, index=False)\n",
    "    combined_table3.to_csv(table3_file, index=False)\n",
    "\n",
    "    print(f\"Combined tables saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "main_directory = './'  # Replace with the path to your main directory\n",
    "output_directory = './DataTables'  # Replace with your desired output directory\n",
    "\n",
    "process_all_csv_files_combined(main_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "# Functions\n",
    "def create_relative_time_minutes(dataframe):\n",
    "    \"\"\"\n",
    "    Creates a new column in the dataframe which records in minutes the time since the earliest date in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: The pandas dataframe that will be modified. Must have an 'EventDateTime' column.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: The modified pandas dataframe\n",
    "    \"\"\"\n",
    "    oldest_date = (pd.to_datetime(dataframe['EventDateTime']).dt.date).min()\n",
    "    dataframe['MinutesSinceStart'] = ((pd.to_datetime(dataframe['EventDateTime']).dt.date - oldest_date) / np.timedelta64(1, 'm'))\n",
    "    dataframe['MinutesSinceStart'] = dataframe['MinutesSinceStart'].astype(int)\n",
    "    return dataframe\n",
    "\n",
    "def sort_by_relative_time(dataframe):\n",
    "    dataframe = dataframe.sort_values(by='MinutesSinceStart')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: False\n",
      "                      BG  InsulinDelivered  FoodDelivered  CarbSize\n",
      "Date                                                               \n",
      "2022-04-27 11:38:14    0              5.00           5.00        75\n",
      "2022-04-27 20:34:41  132              1.96           1.67        25\n",
      "2022-04-28 06:43:30    0              1.07           1.07        16\n",
      "2022-04-28 11:39:14    0              5.00           5.00        75\n",
      "2022-04-28 18:09:54    0              1.07           1.07        16\n",
      "...                  ...               ...            ...       ...\n",
      "2024-10-30 14:29:44  469              7.25           0.00         0\n",
      "2024-10-30 14:38:44    0              2.00           0.00         0\n",
      "2024-10-30 16:08:26  345              0.13           0.00         0\n",
      "2024-10-30 16:37:34  274              5.42           5.42        65\n",
      "2024-10-30 17:44:53  298              0.37           0.00         0\n",
      "\n",
      "[11348 rows x 4 columns]\n",
      "(2228, 24, 4) (2228,) (2203, 24, 4) (2203,)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/g/under/ebottoms/miniconda3/envs/machine-learning/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 - 1s - 34ms/step - loss: 0.1448 - val_loss: 0.1269\n",
      "Epoch 2/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1268 - val_loss: 0.1237\n",
      "Epoch 3/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1259 - val_loss: 0.1232\n",
      "Epoch 4/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1254 - val_loss: 0.1227\n",
      "Epoch 5/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1251 - val_loss: 0.1220\n",
      "Epoch 6/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1246 - val_loss: 0.1214\n",
      "Epoch 7/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1242 - val_loss: 0.1206\n",
      "Epoch 8/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1236 - val_loss: 0.1200\n",
      "Epoch 9/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1232 - val_loss: 0.1197\n",
      "Epoch 10/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1229 - val_loss: 0.1189\n",
      "Epoch 11/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1224 - val_loss: 0.1184\n",
      "Epoch 12/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1221 - val_loss: 0.1181\n",
      "Epoch 13/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1218 - val_loss: 0.1178\n",
      "Epoch 14/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1214 - val_loss: 0.1175\n",
      "Epoch 15/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1212 - val_loss: 0.1173\n",
      "Epoch 16/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1210 - val_loss: 0.1174\n",
      "Epoch 17/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1210 - val_loss: 0.1172\n",
      "Epoch 18/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1209 - val_loss: 0.1172\n",
      "Epoch 19/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1208 - val_loss: 0.1173\n",
      "Epoch 20/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1207 - val_loss: 0.1173\n",
      "Epoch 21/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1207 - val_loss: 0.1173\n",
      "Epoch 22/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1207 - val_loss: 0.1172\n",
      "Epoch 23/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1206 - val_loss: 0.1173\n",
      "Epoch 24/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1205 - val_loss: 0.1172\n",
      "Epoch 25/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1205 - val_loss: 0.1171\n",
      "Epoch 26/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1204 - val_loss: 0.1171\n",
      "Epoch 27/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1204 - val_loss: 0.1172\n",
      "Epoch 28/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1204 - val_loss: 0.1172\n",
      "Epoch 29/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1204 - val_loss: 0.1171\n",
      "Epoch 30/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1203 - val_loss: 0.1171\n",
      "Epoch 31/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1203 - val_loss: 0.1171\n",
      "Epoch 32/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1202 - val_loss: 0.1171\n",
      "Epoch 33/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1203 - val_loss: 0.1170\n",
      "Epoch 34/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1202 - val_loss: 0.1171\n",
      "Epoch 35/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1201 - val_loss: 0.1171\n",
      "Epoch 36/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1202 - val_loss: 0.1171\n",
      "Epoch 37/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1201 - val_loss: 0.1171\n",
      "Epoch 38/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1201 - val_loss: 0.1171\n",
      "Epoch 39/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1200 - val_loss: 0.1172\n",
      "Epoch 40/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1200 - val_loss: 0.1171\n",
      "Epoch 41/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1200 - val_loss: 0.1172\n",
      "Epoch 42/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1199 - val_loss: 0.1172\n",
      "Epoch 43/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1199 - val_loss: 0.1171\n",
      "Epoch 44/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1199 - val_loss: 0.1171\n",
      "Epoch 45/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1198 - val_loss: 0.1174\n",
      "Epoch 46/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1198 - val_loss: 0.1172\n",
      "Epoch 47/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1197 - val_loss: 0.1174\n",
      "Epoch 48/50\n",
      "31/31 - 0s - 6ms/step - loss: 0.1197 - val_loss: 0.1173\n",
      "Epoch 49/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1196 - val_loss: 0.1175\n",
      "Epoch 50/50\n",
      "31/31 - 0s - 7ms/step - loss: 0.1196 - val_loss: 0.1176\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Test MAE: 8.399\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "from pandas import concat\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import sklearn.preprocessing as skl\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "'''\n",
    "The first step in performing a Time Series analysis is visualizing our data over the entire course of time it was collected.\n",
    "This will help us understand the four principle components of a Time Series (Trend, Seasonality, Cycle, and Variation / Noise).\n",
    "Because this data represents someone's blood-glucose levels over time, we expect there to be a relatively flat trend. However, we need to discover seasonality, cycle(s), and noise.\n",
    "'''\n",
    "\n",
    "## Load Bolus Data\n",
    "file_path = './DataTables/Bolus.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['CompletionDateTime'] = pd.to_datetime(df['CompletionDateTime'])\n",
    "df = df.sort_values(by='CompletionDateTime')\n",
    "\n",
    "## Prepare and Clean Data\n",
    "df = df[[\"BG (mg/dL)\", \"CompletionDateTime\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]]\n",
    "# Check for NaN values; there are none\n",
    "print(\"NaN values:\", df.isnull().values.any())\n",
    "# Organize data, make date into an index\n",
    "df.columns = [\"BG\", \"Date\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "columns_titles = [\"Date\", \"BG\", \"InsulinDelivered\", \"FoodDelivered\", \"CarbSize\"]\n",
    "df = df.reindex(columns=columns_titles)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "# Note: we are not using any categorical data\n",
    "values = df.values\n",
    "# specify columns to plot\n",
    "#groups = [0, 1, 2, 3]\n",
    "#i = 1\n",
    "# plot each column\n",
    "#pyplot.figure()\n",
    "#for group in groups:\n",
    "#\tpyplot.subplot(len(groups), 1, i)\n",
    "#\tpyplot.plot(values[:, group])\n",
    "#\tpyplot.title(df.columns[group], y=0.5, loc='right')\n",
    "#\ti += 1\n",
    "#pyplot.show()\n",
    "# normalize features\n",
    "scaler = skl.MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print(df)\n",
    "# Transform to supervised set\n",
    "n_timesteps = 24\n",
    "df_reframed = series_to_supervised(scaled, n_in=n_timesteps, n_out=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "values = df_reframed.values\n",
    "index_2024 = 6893\n",
    "index_last = 11349\n",
    "index_midpoint = index_2024 + ((index_last - index_2024) // 2)\n",
    "train = values[index_2024:index_midpoint,:]\n",
    "test = values[index_midpoint:,:]\n",
    "\n",
    "# Split into input and outputs\n",
    "n_features = 4\n",
    "n_obs = n_timesteps * n_features\n",
    "train_X, train_y = train[:, 0:n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, 0:n_obs], test[:, -n_features]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# design LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit LSTM\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "#pyplot.plot(history.history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='test')\n",
    "#pyplot.legend()\n",
    "#pyplot.show()\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_timesteps*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "mae = np.sqrt(mean_absolute_error(inv_y, inv_yhat))\n",
    "print('Test MAE: %.3f' % mae)\n",
    "\n",
    "\n",
    "#df_Bg = df_Bg.iloc[:, 3:5]\n",
    "#df_Bg.groupby(df_Bg['CompletionDateTime'].dt.hour)['CorrectionDelivered'].mean().plot( kind='bar', rot=0 )\n",
    "#filter = (df_Bg['CompletionDateTime'] > '2024-8') & (df_Bg['CompletionDateTime'] < '2024-11')\n",
    "#print(df_Bg.head())\n",
    "\n",
    "#ax = df_Bg.loc[filter].plot.line(x='CompletionDateTime', y='BG (mg/dL)', rot=0, legend=False, figsize=(20,10))\n",
    "#ax.set_xlabel(\"Date\")\n",
    "#ax.set_ylabel(\"Blood-Glucose Level (mg/dL)\")\n",
    "#ax.set_xbound(lower=pd.Timestamp('2024-01-1 00:00:00'), upper=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if data too noisy, expand the time range that you are predicting. For ex: predict BG over the course of a week instead of a few hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NATE**: Apologies for the small amount of code, but I spent tonight doing a lot of research on Time Series, Pandas' implementation of it, and also the type of machine learning model the professor recommended us to use. He recommended using a type of Neural Network called a \"Recurrent Neural Network\" or RNN. Essentially the idea is this; imagine someone hands you a picture of a cube (no background or anything) and asks you to predict which way it will go next. Obviously you'll just be guessing. But if someone handed you a sequence of pictures of the cube over time, you could take a reasonable guess. This applies to an RNN. The point of an RNN is that it keeps track of previous states, just like the sequence of pictures of the cube. That way, it can make better predictions. However, RNNs suffer from very short-term memory. So, there are more advanced ones called Long-Short Term Memory models or LSTMs. Sufficed to say, they somehow fix the issue somewhat. Anyway, we need this since we have long time periods and also because we want to be able to predict based on the last few hours of information, like getting data from the last 24 hours before making a prediction of BG level. So, I've been following this tutorial (https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/). I'm about halfway through. After it's done, We will do a bunch of hyper parameter tweaks, etc. Rn, the input into the model is gonna be the last 24 hours of BG levels, InsulinDelivered, FoodDelivered, and CarbSize. We can also tweek the amount of hours back the data goes to adjust the accuracy of the model. Anyway, give me any feedback you have, and lemmino if you think there are better input variables. The ones I just listed just seemed the most usable to me because none are categorical and they are within the Bolus dataset which has a ton of entries. Plus they seem intuitively like decent predictors. Anyway, text me anything about it. This should be mostly done by tonight (12/5/24)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
